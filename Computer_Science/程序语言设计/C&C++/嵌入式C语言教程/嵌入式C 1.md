# 嵌入式C语言——ARM汇编架构大纲

## 架构说明

​	常见的计算机架构如下所示：

| 特点       | 冯诺依曼架构                           | 哈佛架构                            | 混合架构                         |
| ---------- | -------------------------------------- | ----------------------------------- | -------------------------------- |
| 存储方式   | 统一存储（程序和数据共享同一存储空间） | 分开存储（程序和数据分别存储）      | 同时具备统一和分开存储的特性     |
| 访问速度   | 存储瓶颈（数据和指令不能同时访问）     | 更快（数据和指令可以同时访问）      | 平衡（在不同情况下选择存储方式） |
| 设计复杂性 | 较简单                                 | 较复杂                              | 复杂性介于两者之间               |
| 应用场景   | 通用计算机                             | 嵌入式系统和信号处理                | 适合对性能和灵活性有要求的系统   |
| 实例       | 大多数通用计算机                       | DSP（数字信号处理器）、某些微控制器 | 现代处理器（如某些x86架构的CPU） |

​	现代计算机已经输入混合架构了。所以大多数下我们需要进一步的拿到框图进行分析。

## Cache机制

​	计算机的Cache（缓存）体系是现代计算机架构中至关重要的组成部分，它旨在通过减少CPU访问主存储器（RAM）的次数，提高数据处理速度。Cache的设计和实现直接影响系统的性能、能效以及整体计算能力。现代计算机体系架构首先要求CPU首先去Cache中读取数据，如果我们的缓存成功命中了，那就是大成功，不用访问内存就可以拿到数据，大大节约了时钟周期。

​	CPU的Cache也是分级的，常见的有三级Cache（L1、L2、L3），这种多层次的Cache设计，使得系统能够在不同层次之间平衡速度与容量：

- L1 Cache：位于CPU内部，速度最快，容量较小（通常为32KB到256KB）。L1 Cache分为数据Cache和指令Cache，分别存储数据和指令。
- L2 Cache：通常也在CPU内部，容量大于L1（一般为256KB到8MB），速度略慢于L1，但仍然比主存快。
- L3 Cache：通常共享于多个CPU核心之间，容量更大（几MB到几十MB），速度相对较慢，但仍优于主存。

​	Cache的工作原理主要基于“局部性原则”，包括时间局部性和空间局部性：时间局部性：如果某个数据被访问，那么在不久的将来，它可能会再次被访问。空间局部性：如果某个数据被访问，那么相邻的数据很可能会被访问。

### 关于Cache本身

- Cache行（Cache Line）：Cache中存储的基本单位，通常包含一个数据块及其相关信息，如标签（Tag）和有效位（Valid Bit）。
- 标签（Tag）：用于标识Cache行中的数据来源，帮助判断该行数据是否有效。
- 有效位（Valid Bit）：指示该Cache行是否包含有效数据。

当Cache满时，需要选择一个Cache行进行替换，常见的替换策略包括：

- 最近最少使用（LRU）：替换最久未被使用的Cache行。
- 先进先出（FIFO）：按照数据进入Cache的顺序进行替换。
- 随机替换：随机选择一个Cache行进行替换。
- 这些策略的选择会影响Cache的命中率和系统性能。

​	在多核处理器中，Cache一致性是一个重要问题。多个核心可能会有各自的Cache，这就需要确保在多个Cache中对同一数据的修改能够及时反映。常见的Cache一致性协议有：

- MESI协议：每个Cache行有四种状态（修改、独占、共享、无效），通过状态转换来保持一致性。
- MOESI协议：在MESI的基础上增加了“拥有”状态，优化了一些场景下的性能。

Cache性能评估：Cache的性能通常通过命中率（Hit Rate）来评估。命中率是Cache中找到所需数据的比例。提高命中率的方式包括：

- 增大Cache容量：可以容纳更多数据，通常可以提高命中率。

- 优化Cache行大小：适当调整Cache行的大小，平衡访问速度与数据局部性。
- 调整替换策略：根据具体应用场景选择合适的替换策略。

随着处理器核心数量的增加和应用需求的多样化，Cache设计面临着多重挑战：

- 能耗问题：Cache的能耗逐渐成为系统能耗的重要组成部分，如何在保证性能的同时降低能耗是一个关键问题。

- 延迟问题：尽管Cache相较于主存速度较快，但在高性能计算中，如何进一步减少延迟仍是研究的重点。
- 多核架构的复杂性：随着多核和多处理器系统的普及，Cache一致性和共享Cache的设计变得更加复杂，需要更有效的协议和算法。

对于一个CPU内部的多个核，每一个核都有自己独立的L1 Cache，包括D缓存和I缓存。L2Cache被簇CPU所共享，L3是被全局的共享。

### 一些处理器没有Cache的原因：

- 设计简化：对于某些嵌入式系统或低功耗设备，省略Cache可以简化处理器设计，降低生产成本和功耗。

- 应用需求：某些应用对实时性要求高，直接访问主存可以避免Cache带来的不确定延迟。
- 小规模系统：在资源有限的设备上，Cache可能会占用过多空间，因此选择不使用。
- 特定架构：某些处理器架构（如某些RISC处理器）可能设计为直接从主存获取数据，以优化性能和成本。
- 低频使用场景：在数据访问频率较低的场合，Cache的引入可能并不会显著提升性能。

### 流水线的基本概念

流水线的核心思想是将指令的执行分解为若干个阶段，每个阶段执行一部分工作。这种分解允许处理器在每个时钟周期中同时处理多条指令的不同部分。典型的指令执行流程可以分为以下几个阶段：

- 取指阶段（IF）：从内存中读取指令，并将其放入指令寄存器。

- 指令解码阶段（ID）：对取出的指令进行解码，识别操作类型和操作数，准备执行。
- 执行阶段（EX）：执行指令的主要操作，包括算术逻辑运算、地址计算等。
- 访存阶段（MEM）：对需要访问内存的指令进行操作，读取或写入数据。
- 写回阶段（WB）：将执行结果写回到寄存器或内存中。

在理想情况下，每个阶段都可以在一个时钟周期内完成。通过这种方式，处理器能够在每个周期内发射新指令，从而实现更高的指令吞吐量。

流水线的优势
流水线技术带来了显著的性能提升。首先，通过并行处理，流水线能够将指令执行的时间缩短。例如，假设每条指令需要五个周期才能完成，流水线允许在五个周期内完成五条指令的执行，相比于顺序执行的处理方式，吞吐量得到了极大的提高。

其次，流水线的有效利用能够减少处理器的空闲时间。传统的非流水线处理器在执行每条指令时，所有的资源都在等待当前指令完成，而流水线则允许不同的指令在不同的阶段同时进行，从而提高了资源的利用率。

流水线的挑战
尽管流水线技术带来了诸多优势，但其实现也面临许多挑战。最主要的问题是指令之间的依赖关系。指令之间可能存在数据依赖关系，即某条指令的执行结果可能被后续指令所依赖。这种依赖会导致流水线中的“气泡”，即某个阶段因等待数据而停顿，影响整体性能。

控制依赖也是流水线设计中的一个重要问题。分支指令（如条件跳转）会使得处理器在决定执行哪条指令时出现不确定性。处理器必须在知道条件结果之前等待，这会导致流水线停顿，降低吞吐量。为了解决这些问题，现代处理器通常会采用分支预测技术，通过预测分支的结果来减少停顿的发生。

### 流水线的优化技术

- 动态调度：动态调度允许处理器在运行时重新安排指令的执行顺序，以减少由于数据依赖而造成的停顿。这种方法通常通过保留指令的执行顺序，允许某些指令在等待数据时被重新调度。
- 分支预测：分支预测通过分析历史分支行为，预测未来分支的结果，从而减少由于控制依赖带来的停顿。现代处理器采用多种分支预测算法，以提高预测的准确性。
- 超标量执行：超标量处理器能够在同一周期内发射多条指令，通过增加并行度来提高性能。超标量设计通常需要更复杂的指令调度和资源管理机制。
- 乱序执行：乱序执行允许指令在不违反程序顺序的情况下，按照资源的可用性进行执行。这种方法能够更有效地利用流水线资源，减少气泡的影响。



