# Pytorch å­¦ä¹ è·¯ç¨‹

â€‹	Pytorchæ˜¯é‡è¦çš„äººå·¥æ™ºèƒ½æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚æ—¢ç„¶å·²ç»ç‚¹è¿›æ¥ï¼Œæˆ‘ä»¬å°±è¯¦ç»†çš„ä»‹ç»ä¸€ä¸‹å•¥æ˜¯Pytorch

>[PyTorch](https://pytorch.org/)

- å¸Œæœ›å°†å…¶ä»£æ›¿ Numpy æ¥åˆ©ç”¨ GPUs çš„å¨åŠ›ï¼›
- ä¸€ä¸ªå¯ä»¥æä¾›æ›´åŠ çµæ´»å’Œå¿«é€Ÿçš„æ·±åº¦å­¦ä¹ ç ”ç©¶å¹³å°ã€‚

## ä¸‹è½½Pytorch

â€‹	ä¸å¿…ç€æ€¥æ‹…å¿ƒæˆ‘ä»¬ä¸‹å•¥ç‰ˆæœ¬ï¼ŒPytorchå®˜ç½‘å·²ç»ç»™å‡ºäº†ä¸€ä¸ªè‰¯å¥½çš„è§£å†³æ–¹æ¡ˆï¼š

![image-20240419180803360](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240419180803360.png)

â€‹	è¯·æ ¹æ®è‡ªå·±çš„ç½‘ç«™ç»™å‡ºçš„æ–¹æ¡ˆè¿›è¡Œé€‰æ‹©ï¼ä¸è¦æŠ„æˆ‘çš„ï¼

â€‹	å¯ä»¥å¤åˆ¶åˆ°Pycharmä¸­ï¼Œç¡®å®šå¥½è‡ªå·±çš„è™šæ‹Ÿç¯å¢ƒä¹‹åï¼Œå°±å¯ä»¥æ„‰å¿«çš„åœ¨ç»ˆç«¯æ‰§è¡Œç½‘ç«™æ¨ä»‹çš„é…ç½®.

> å¯ä»¥åœ¨PackageåŒ…ä¸­é€‰æ‹©è‡ªå·±çš„åŒ…ç®¡ç†ï¼šå¦‚æœä½ çš„ç¯å¢ƒæ˜¯condaç¯å¢ƒï¼Œæˆ‘ä¸ªäººæ¨ä»‹ä½¿ç”¨condaæ¥ä¸‹ï¼ˆæ–¹ä¾¿ç®¡ç†ï¼‰

â€‹	ç­‰å¾…åŠä¸ªå°æ—¶ï¼Œæˆ‘ä»¬ä¸‹å¥½äº†ä¹‹åï¼Œï¼Œå°±å¯ä»¥ä½¿ç”¨è¿™ä¸ªä»£ç è·‘ä¸€ä¸‹ï¼š

> åœ¨Pycharmçš„Pythonæ§åˆ¶å°ä¸Š

```python
import torch
torch.__version__
```

> ä¹‹åæˆ‘ä»¬å°†ä¼šåœ¨æ§åˆ¶å°ä¸Šå°è¯•æˆ‘ä»¬çš„ä»£ç ï¼Œè¿™é‡Œå°±ä¸èµ˜è¿°äº†

### å…¥é—¨å°è¯•

â€‹	æˆ‘ä»¬éšæ„çš„è¯•ä¸€è¯•ä¸€äº›APIï¼š

â€‹	æˆ‘ä»¬å¯ä»¥å¾ˆè½»æ¾çš„åˆ›å»ºä¸€ä¸ªçŸ©é˜µï¼š

> [torch.empty â€” PyTorch 2.2 documentation](https://pytorch.org/docs/stable/generated/torch.empty.html#torch-empty)

```
x = torch.empty(5, 3)
x
```

```
tensor([[1.4767e+20, 1.6816e-42, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00]])
```

â€‹	æˆ‘ä»¬å°±ä¼šåˆ›å»ºä¸€ä¸ªç»™å®šå¤§å°çš„torch:ä»–çš„å€¼æ˜¯æœªåˆå§‹åŒ–çš„ï¼ˆä½ å¯ä»¥åå¤æ‰§è¡ŒæŸ¥çœ‹ç»“æœï¼Œä½ ä¼šå‘ç°ç»“æœå¯èƒ½æ¯ä¸€æ¬¡éƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼‰

â€‹	æˆ‘ä»¬å¯ä»¥å¾ˆè½»æ¾çš„åˆ›å»ºä¸€ä¸ªéšæœºçŸ©é˜µï¼š

> [torch.rand â€” PyTorch 2.2 documentation](https://pytorch.org/docs/stable/generated/torch.rand.html#torch-rand)

```
x = torch.rand(5, 3)
x
```

```
tensor([[0.7140, 0.1131, 0.6945],
        [0.8082, 0.6078, 0.5954],
        [0.9646, 0.6500, 0.8988],
        [0.4161, 0.1819, 0.3053],
        [0.1953, 0.3988, 0.9033]])
```

â€‹	ç”±æ­¤å¯è§ï¼Œä»–ä¼šéšæœºçš„ç”Ÿæˆä¸€äº›ä»‹äº0å’Œ1ä¹‹é—´çš„éšæœºå€¼

>[torch.zeros â€” PyTorch 2.2 documentation](https://pytorch.org/docs/stable/generated/torch.zeros.html)

```
x = torch.zeros(5, 3, dtype=torch.long)
x
```

â€‹	å°†è¿”å›ç»™æˆ‘ä»¬ä¸€ä¸ªå…¨0çš„çŸ©é˜µ

â€‹	æˆ‘ä»¬è¿˜å¯ä»¥å‡çº§å·²æœ‰çš„æ•°ç»„ç»“æ„ï¼š

>[torch.tensor â€” PyTorch 2.2 documentation](https://pytorch.org/docs/stable/generated/torch.tensor.html)

```
x = torch.tensor([5.5, 3])
x
```

```
tensor([5.5000, 3.0000])
```

â€‹	å½“ç„¶å¯ä»¥ä½¿ç”¨sizeæŸ¥çœ‹torchçš„å¤§å°

```
x.size()
```

â€‹	è¿˜å¯ä»¥å¯¹ä¹‹è¿›è¡Œç®€å•çš„æ“ä½œï¼š

```
y = torch.rand(5, 3)
x + y
# ç­‰ä»·æ“ä½œï¼štorch.add(x, y)
```

```
tensor([[1.1685, 1.4813, 1.1385],
        [1.4541, 1.4664, 1.4721],
        [1.5987, 1.1817, 1.3344],
        [1.2923, 1.8951, 1.8134],
        [1.8740, 1.7830, 1.7349]], dtype=torch.float64)
```

â€‹	è¿˜å¯ä»¥åŒä¸€èˆ¬çš„Pythoné‚£æ ·è¿›è¡Œç´¢å¼•

```
print(x)
x[:, 1]
```

```
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
```

â€‹	è¿˜å¯ä»¥å˜æ¢ç»´åº¦

>[torch.Tensor.view â€” PyTorch 2.2 documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch-tensor-view)

PyTorchä¸­çš„view( )å‡½æ•°ç›¸å½“äºnumpyä¸­çš„resize( )å‡½æ•°ï¼Œéƒ½æ˜¯ç”¨æ¥é‡æ„(æˆ–è€…è°ƒæ•´)å¼ é‡ç»´åº¦çš„ï¼Œç”¨æ³•ç¨æœ‰ä¸åŒã€‚

```
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8) 
print(x.size(), y.size(), z.size())
```

â€‹	è¿˜æ”¯æŒåŒå…¶ä»–åº“çš„ååŒæ“ä½œï¼š

```
a = torch.ones(5)
b = a.numpy()
b
```

```
array([1., 1., 1., 1., 1.], dtype=float32)
```

```
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
b
```

```
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
```

## å‡ ç§å¸¸è§çš„Tensor

> [torch.Tensor â€” PyTorch 2.2 documentation](https://pytorch.org/docs/stable/tensors.html)

â€‹	æˆ‘ä»¬çš„Tensorå«å¼ é‡ï¼Œå›å¿†çº¿æ€§ä»£æ•°ï¼Œæˆ‘ä»¬çš„å¼ é‡æœ‰ç»´åº¦ï¼Œæˆ‘ä»¬çš„ç»´åº¦å¯ä»¥ä»0ä¸Šå‡åˆ°ï¼š

```
0: scalar		# æ ‡é‡
1: vector		# å‘é‡
2: matrix
3: n-dim tensor
```

### Scalar

â€‹	é€šå¸¸å°±æ˜¯ä¸€ä¸ªæ•°å€¼ï¼š

```
x = tensor(42.)
x
```

â€‹	ä½ å°±ä¼šå‘ç°ç»“æœå®é™…ä¸Šå°±æ˜¯å°è£…èµ·æ¥çš„ä¸€ä¸ªæ•°å­—ï¼š

```
tensor(42.)
```

â€‹	ä½¿ç”¨dimæ–¹æ³•å¯ä»¥æŸ¥çœ‹è¿™ä¸ªå¼ é‡çš„ç»´åº¦ï¼š

```
x.dim()
```

```
0
```

â€‹	å¯ä»¥ç®€å•ä½¿ç”¨æ ‡é‡ä¹˜æ³•ï¼Œè·Ÿçº¿æ€§ä»£æ•°å®šä¹‰çš„ä¹˜æ³•å®Œå…¨ä¸€è‡´ï¼š

```
2 * x
```

```
tensor(84.)
```

â€‹	å¯¹äºæ ‡é‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨itemæ–¹æ³•æå–é‡Œé¢çš„å€¼

```
x.item()
```

â€‹	ä½†æ˜¯å»ºè®®åˆ¤æ–­itemçš„ç»´åº¦é€‰ç”¨è¿™ä¸ªæ–¹æ³•ï¼Œå› ä¸ºå¯¹äºå‘é‡ï¼Œè¿™ä¸ªæ–¹æ³•ä¼šæŠ›error

```
y = torch.tensor([3, 4])
y.item()
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[9], line 2
      1 y = torch.tensor([3, 4])
----> 2 y.item()

RuntimeError: a Tensor with 2 elements cannot be converted to Scalar
```

### Vector

â€‹	ä¾‹å¦‚ï¼š `[-5., 2., 0.]`ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸æŒ‡ç‰¹å¾ï¼Œä¾‹å¦‚è¯å‘é‡ç‰¹å¾ï¼ŒæŸä¸€ç»´åº¦ç‰¹å¾ç­‰

### Matrix

â€‹	æˆ‘ä»¬æ·±åº¦å­¦ä¹ çš„è®¡ç®—å¤šæ¶‰åŠçŸ©é˜µï¼š

```
M = tensor([[1., 2.], [3., 4.]])
M
```

```
tensor([[1., 2.],
        [3., 4.]])
```

â€‹	çŸ©é˜µå¯ä»¥è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œä½†æ˜¯è¦æ±‚æ»¡è¶³çº¿æ€§ä»£æ•°ä¸‹çŸ©é˜µçš„ä¹˜æ³•è§„åˆ™ï¼š

```
N = tensor([1, 2, 3])
M.matmul(N)
```

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[12], line 2
      1 N = tensor([1, 2, 3])
----> 2 M.matmul(N)

RuntimeError: size mismatch, got input (2), mat (2x2), vec (3)
```

![5cd99a73f8ce4494ad86852e_arraychart.jpg (3540Ã—3187) (webflow.com)](https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5cd99a73f8ce4494ad86852e_arraychart.jpg)

## AutoGradæœºåˆ¶

> [æ·±åº¦è§£æ PyTorch Autogradï¼šä»åŸç†åˆ°å®è·µ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/676009630)
>
> [Pytorch autograd,backwardè¯¦è§£ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/83172023)

â€‹	å‚è€ƒè¿™ä¸¤ä¸ªåšå®¢ï¼Œæˆ‘æ¥å†™å†™æˆ‘çš„ç†è§£ã€‚æˆ‘ä»¬æ„å»ºçš„æ˜¯åŸºäºå¼ é‡çš„å‡½æ•°ç®—å­ï¼š
$$
f = f(X, Y, Z, ...)
$$
â€‹	ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å¯¼ï¼Œé¦–å…ˆå°±è¦æ€è€ƒï¼Œå¯¹äºå¤šå¼ é‡çš„å‡½æ•°ï¼Œè·Ÿå¤šå˜é‡å‡½æ•°ä¸€æ ·ï¼Œä¸€äº›å˜é‡æ˜¯æˆ‘ä»¬è¿™æ¬¡è¿ç®—ä¸­éœ€è¦è¢«æ±‚å¯¼çš„ï¼Œä¸€äº›ä¸æ˜¯ï¼Œè¿™æ ·ï¼Œæˆ‘ä»¬å°±éœ€è¦ä½¿ç”¨Tensorçš„required_gradå‚æ•°æœºåˆ¶ï¼š

```
x = torch.randn(3,4,requires_grad=True)
x
```

â€‹	è¿™æ ·æˆ‘ä»¬çš„xåœ¨åç»­å‚ä¸å‡½æ•°è¿ç®—çš„æ—¶å€™ï¼Œåœ¨æˆ‘ä»¬åå‘ä¼ æ’­çš„æ—¶å€™å°±ä¼šå‚ä¸æ±‚å¯¼è¿ç®—ã€‚

> ä¸€äº›å‚æ•°çš„è§£é‡Š

- `data`: å³å­˜å‚¨çš„æ•°æ®ä¿¡æ¯
- `requires_grad`: è®¾ç½®ä¸º`True`åˆ™è¡¨ç¤ºè¯¥Tensoréœ€è¦æ±‚å¯¼
- `grad`: è¯¥Tensorçš„æ¢¯åº¦å€¼ï¼Œæ¯æ¬¡åœ¨è®¡ç®—backwardæ—¶éƒ½éœ€è¦å°†å‰ä¸€æ—¶åˆ»çš„æ¢¯åº¦å½’é›¶ï¼Œå¦åˆ™æ¢¯åº¦å€¼ä¼šä¸€ç›´ç´¯åŠ ï¼Œè¿™ä¸ªä¼šåœ¨åé¢è®²åˆ°ã€‚
- `grad_fn`: å¶å­èŠ‚ç‚¹é€šå¸¸ä¸ºNoneï¼Œåªæœ‰ç»“æœèŠ‚ç‚¹çš„grad_fnæ‰æœ‰æ•ˆï¼Œç”¨äºæŒ‡ç¤ºæ¢¯åº¦å‡½æ•°æ˜¯å“ªç§ç±»å‹ã€‚ä¾‹å¦‚ä¸Šé¢ç¤ºä¾‹ä»£ç ä¸­çš„`y.grad_fn=<PowBackward0 at 0x213550af048>, z.grad_fn=<AddBackward0 at 0x2135df11be0>`
- `is_leaf`: ç”¨æ¥æŒ‡ç¤ºè¯¥Tensoræ˜¯å¦æ˜¯å¶å­èŠ‚ç‚¹ã€‚

â€‹	ç°åœ¨æˆ‘ä»¬å¼•å…¥å‡½æ•°ç®—å­ï¼š

```
b = torch.randn(3,4,requires_grad=True)
# print(b)
t = x + b
t
```

â€‹	æˆ‘ä»¬å®é™…ä¸Šå®Œæˆçš„æ˜¯ä¸¤ä¸ªå¼ é‡çš„ç›¸åŠ ï¼Œç°åœ¨æˆ‘ä»¬å°±çŸ¥é“ï¼Œtä½œä¸ºä¸€ä¸ªç»“æœï¼Œå‘ç”Ÿäº†ä¸¤ä¸ªå¼ é‡çš„ç›¸åŠ ï¼š

```
tensor([[ 1.2804, -1.8381,  0.0068, -0.3126],
        [-0.4901,  1.5733, -1.1383,  1.4996],
        [ 1.9931, -0.7548, -1.1527, -1.1703]], grad_fn=<AddBackward0>)# çœ‹åé¢è¿™ä¸ªï¼Œè¿™ä¸ªè¯´æ˜ç¨åæˆ‘ä»¬åå‘ä¼ æ’­çš„æ—¶å€™ä½¿ç”¨AddBackwardç®—å­
```

â€‹	ä½¿ç”¨`y.backward()`è¿›è¡Œåå‘ä¼ æ’­ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬å¦‚ä½•æŸ¥çœ‹å‚ä¸è¿ç®—çš„å¼ é‡çš„æ¢¯åº¦å‘¢ï¼Œç­”æ¡ˆæ˜¯ï¼š

```
print(x.grad)
print(b.grad)
```

â€‹	å¯ä»¥æ³¨æ„åˆ°ï¼šæˆ‘ä»¬æ±‚ä¸€æ¬¡`y.backward()`ï¼Œè¿™ä¸ªç»“æœå°±ä¼šç´¯åŠ ä¸€æ¬¡ã€‚

â€‹	æ³¨æ„åˆ°ï¼Œä¸€äº›å¼ é‡ä¸æ˜¯æˆ‘ä»¬å®šä¹‰å‡ºæ¥çš„è€Œæ˜¯ç®—å‡ºæ¥çš„ï¼Œä»£è¡¨æ€§çš„å°±æ˜¯t,åä¹‹å‰©ä¸‹çš„æ˜¯å‚ä¸åŸºç¡€è¿ç®—çš„xå’Œb

```
print(x.is_leaf, b.is_leaf, t.is_leaf)
True True False
```

â€‹	è¿™æ ·æˆ‘ä»¬å°±ä¸ä¼šå¯¹å¶å­å‘é‡æ±‚å¯¼äº†ï¼ä»–ä»¬å°±æ˜¯åŸºç¡€çš„å˜é‡ã€‚

### çº¿æ€§å›å½’å°è¯•

â€‹	å•¥æ˜¯çº¿æ€§å›å½’å‘¢ï¼Œæˆ‘çš„ç†è§£æ˜¯ï¼šä½¿ç”¨çº¿æ€§çš„å‡½æ•°ï¼ˆå¦‚æœä¸ç†è§£ï¼Œé‚£å°±æ˜¯`y = kx + b`ï¼‰æ‹Ÿåˆæ•°æ®ã€‚æˆ‘ä»¬ä»ç®€å•çš„çº¿æ€§æ‹Ÿåˆæ¥ã€‚

â€‹	ç”Ÿæˆä¸€ç»„`(x, y)`å¯¹

```
import numpy as np
x_values = [i for i in range(11)]
x_train = np.array(x_values, dtype=np.float32)
x_train = x_train.reshape(-1, 1)
x_train.shape
x_train
```

```
array([[ 0.],
       [ 1.],
       [ 2.],
       [ 3.],
       [ 4.],
       [ 5.],
       [ 6.],
       [ 7.],
       [ 8.],
       [ 9.],
       [10.]], dtype=float32)
```

```
y_values = [2*i + 1 for i in x_values]
y_train = np.array(y_values, dtype=np.float32)
y_train = y_train.reshape(-1, 1)
y_train.shape
y_train
```

```
array([[ 1.],
       [ 3.],
       [ 5.],
       [ 7.],
       [ 9.],
       [11.],
       [13.],
       [15.],
       [17.],
       [19.],
       [21.]], dtype=float32)
```

â€‹	ç°åœ¨æˆ‘ä»¬ä½¿ç”¨torchæ¡†æ¶ä¸‹çš„çº¿æ€§å›å½’ï¼š

```
import torch
import torch.nn as nn
```

```
class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)  

    def forward(self, x):
        out = self.linear(x) # å‘å‰ä¼ æ’­
        return out
```

â€‹	è¿™æ ·æˆ‘ä»¬å°±å®Œæˆäº†ä¸€ä¸ªæœ€ç®€å•çš„æ¨¡å‹

```
input_dim = 1
output_dim = 1

model = LinearRegressionModel(input_dim, output_dim)
model
```

```
LinearRegressionModel(
  (linear): Linear(in_features=1, out_features=1, bias=True)
)
```

```
epochs = 1000			# è®­ç»ƒè®ºæ•°
learning_rate = 0.01	# å­¦ä¹ é€Ÿç‡
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)	# éšæœºæ¢¯åº¦ä¸‹é™
criterion = nn.MSELoss()	# æ­£åˆ™åŒ–æƒ©ç½šç³»æ•°
```

â€‹	åœ¨è¿™é‡Œæˆ‘ä»¬è¿›è¡Œè®­ç»ƒ

```
for epoch in range(epochs):
    epoch += 1
    # æ³¨æ„è½¬è¡Œæˆtensor
    inputs = torch.from_numpy(x_train)
    labels = torch.from_numpy(y_train)

    # æ¢¯åº¦è¦æ¸…é›¶æ¯ä¸€æ¬¡è¿­ä»£
    optimizer.zero_grad() 

    # å‰å‘ä¼ æ’­
    outputs = model(inputs)

    # è®¡ç®—æŸå¤±
    loss = criterion(outputs, labels)

    # è¿”å‘ä¼ æ’­
    loss.backward()

    # æ›´æ–°æƒé‡å‚æ•°
    optimizer.step()
    if epoch % 50 == 0:
        print('epoch {}, loss {}'.format(epoch, loss.item()))
```

â€‹	æˆ‘ä»¬å¯ä»¥è¿™æ ·å¾—åˆ°é¢„æµ‹çš„å€¼ï¼š

```
predicted = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()
predicted
```

â€‹	å¦‚ä½•å­˜å–æ¨¡å‹å‘¢ï¼š

```
torch.save(model.state_dict(), 'model.pkl')
model.load_state_dict(torch.load('model.pkl'))
```

â€‹	ä¹Ÿå¯ä»¥ä½¿ç”¨GPUè®­ç»ƒ

```
import torch
import torch.nn as nn
import numpy as np


class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)  

    def forward(self, x):
        out = self.linear(x)
        return out

input_dim = 1
output_dim = 1

model = LinearRegressionModel(input_dim, output_dim)

# åœ¨è¿™é‡Œï¼Œç›´æ¥æ‰”åˆ°GPUå°±è¡Œ
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)


criterion = nn.MSELoss()


learning_rate = 0.01

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

epochs = 1000
for epoch in range(epochs):
    epoch += 1
    inputs = torch.from_numpy(x_train).to(device)
    labels = torch.from_numpy(y_train).to(device)

    optimizer.zero_grad() 

    outputs = model(inputs)

    loss = criterion(outputs, labels)

    loss.backward()

    optimizer.step()

    if epoch % 50 == 0:
        print('epoch {}, loss {}'.format(epoch, loss.item()))
```

## ä½¿ç”¨hubæ¨¡å—

> [torch.hub â€” PyTorch 2.2 documentation](https://pytorch.org/docs/stable/hub.html)

â€‹	**Pytorch Hubæ˜¯ä¸€ä¸ªå¸®åŠ©ç ”ç©¶è€…å®ç°æ¨¡å‹å†ç°ã€å¿«é€Ÿæ¨ç†éªŒè¯çš„é¢„è®­ç»ƒæ¨¡å‹åº“ä¸ä¸€å¥—ç›¸å…³çš„APIæ¡†æ¶**ã€‚æ”¯æŒè¿œç¨‹ä»githubä¸Šä¸‹è½½æŒ‡å®šæ¨¡å‹ã€ä¸Šä¼ ä¸åˆ†äº«è®­ç»ƒå¥½çš„æ¨¡å‹ã€æ”¯æŒä»æœ¬åœ°åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€è‡ªå®šä¹‰æ¨¡å‹ã€‚æ”¯æŒæ¨¡å‹è¿œç¨‹åŠ è½½ä¸æœ¬åœ°æ¨ç†ã€å½“å‰Pytorch Hubå·²ç»å¯¹æ¥åˆ°Torchvisionã€YOLOv5ã€YOLOv8ã€pytorchvideoç­‰è§†è§‰æ¡†æ¶

â€‹	äººè¯ï¼šæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨æ“ä½œè¿™äº›APIç›´æ¥å«–è®¾ç½®å¥½çš„æ¨¡å‹ç›´æ¥ç”¨ã€‚

â€‹	æˆ‘ä»¬å¯ä»¥å‰å¾€[Pytorch Hub](https://pytorch.org/hub)å°è¯•ï¼Œæœç´¢ä½ æ„Ÿå…´è¶£çš„æ¨¡å‹ï¼šæ¥ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯¹deeplabv3_resnet101ï¼Œå°±å¯ä»¥æœç´¢åˆ°Tutorial:

[Deeplabv3 | PyTorch](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)

```
import torch
model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)
# or any of these variants
# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)
# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)
model.eval()
```

â€‹	è¿™ä¸ªæ—¶å€™ä»–ä¼šä¸‹è½½æ¨¡å‹ï¼ˆé»˜è®¤ä¿å­˜åœ¨ç”¨æˆ·æ–‡ä»¶å¤¹ä¸‹çš„`C:/User/.cache/torch/`ä¸‹ï¼‰

â€‹	ä¹‹åä¸‹è½½æ•°æ®é›†ï¼š

```
# Download an example image from the pytorch website
import urllib
url, filename = ("https://github.com/pytorch/hub/raw/master/images/deeplab1.png", "deeplab1.png")
try: urllib.URLopener().retrieve(url, filename)
except: urllib.request.urlretrieve(url, filename)
```

> å¦‚æœç½‘ç»œä¸å¥½ï¼Œè¯·æ‰‹åŠ¨åˆ°åœ°å€ä¸‹è½½ï¼æ”¾åˆ°æŒ‡å®šä½ç½®

â€‹	ç„¶åå¤„ç†å®ƒï¼š

```
# sample execution (requires torchvision)
from PIL import Image
from torchvision import transforms
# å®šä¹‰transformç®—å­
input_image = Image.open(filename)
input_image = input_image.convert("RGB")
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
# é¢„å¤„ç†
input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model

# move the input and model to GPU for speed if available
if torch.cuda.is_available():
    input_batch = input_batch.to('cuda')
    model.to('cuda')

with torch.no_grad():
    output = model(input_batch)['out'][0]
output_predictions = output.argmax(0)
```

â€‹	æŸ¥çœ‹æ•ˆæœå¦‚ä½•

```
# create a color pallette, selecting a color for each class
palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])
colors = torch.as_tensor([i for i in range(21)])[:, None] * palette
colors = (colors % 255).numpy().astype("uint8")

# plot the semantic segmentation predictions of 21 classes in each color
r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
r.putpalette(colors)

import matplotlib.pyplot as plt
plt.imshow(r)
plt.show()
```

![image-20240419201652254](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240419201652254.png)

åˆ†ç±»æˆåŠŸã€‚

# Pytorchå­¦ä¹ è·¯ç¨‹ 2ï¼šç¥ç»ç½‘ç»œåˆ†ç±»ä¸å®æˆ˜

## ç¥ç»ç½‘ç»œåˆ†ç±»

â€‹	æˆ‘ä»¬ä»¥æœ€ä¸ºç»å…¸çš„MNISTæ•°æ®é›†ä¸ºä¾‹å­ï¼Œè¯†åˆ«æ‰‹å†™çš„æ•°å­—ï¼ˆè¿™ä¸ªå¤ªç»å…¸äº†ï¼‰

### ä¸‹è½½æ•°æ®é›†

â€‹	æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥å°±æ˜¯ä¸‹è½½æ•°æ®é›†å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼š

```python
%matplotlib inline 

from pathlib import Path
import requests

DATA_PATH = Path("data")
PATH = DATA_PATH / "mnist"

PATH.mkdir(parents=True, exist_ok=True)	# åˆ›å»ºä¸€ä¸ªè¿™æ ·çš„æ–‡ä»¶å¤¹ä»¥

URL = "http://deeplearning.net/data/mnist/"
FILENAME = "mnist.pkl.gz"

if not (PATH / FILENAME).exists():
        content = requests.get(URL + FILENAME).content
        (PATH / FILENAME).open("wb").write(content)
        
```

> 1.[Pythonç»˜å›¾é—®é¢˜ï¼šMatplotlibä¸­%matplotlib inlineæ˜¯ä»€ä¹ˆã€å¦‚ä½•ä½¿ç”¨ï¼Ÿ_%matplotlib inline mial-CSDNåšå®¢](https://blog.csdn.net/liangzuojiayi/article/details/78183783)ï¼Œè¿™ä¸ªåšå®¢å¯ä»¥å›ç­”ä¸ºä»€ä¹ˆæ•´ä¸€ä¸ª`%matplotlib inline`
>
> æˆ‘ä»¬å®é™…ä¸Šåœ¨è¿™é‡Œåšçš„å°±æ˜¯å°†æ•°æ®é›†å†™å…¥åˆ°æˆ‘ä»¬æŒ‡å®šçš„åŒ…ã€‚ä½¿ç”¨çš„æ˜¯ç½‘ç»œåŒ…è¯·æ±‚

â€‹	ä¸‹ä¸€æ­¥å°±æ˜¯å°†ä¸‹è½½çš„æ•°æ®å†™å…¥å¯¹åº”çš„åœ°æ–¹ï¼šæ€ä¹ˆå†™å‘¢ï¼Ÿä½¿ç”¨Pickelæ¨¡å—å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼š

> å•¥æ˜¯Pickle:å¯ä»¥ç®€å•çš„ç†è§£ä¸ºå¯¹æ–‡ä»¶ç›´æ¥å†™å…¥è¯»å‡ºäºŒè¿›åˆ¶æµï¼Œçœå»æˆ‘ä»¬æ“å¿ƒ
>
> æˆ–è€…è¯´ï¼ŒPickleè®©æ•°æ®æŒä¹…åŒ–ä¿å­˜ã€‚pickleæ¨¡å—æ˜¯Pythonä¸“ç”¨çš„æŒä¹…åŒ–æ¨¡å—ï¼Œå¯ä»¥æŒä¹…åŒ–åŒ…æ‹¬è‡ªå®šä¹‰ç±»åœ¨å†…çš„å„ç§æ•°æ®ï¼Œæ¯”è¾ƒé€‚åˆPythonæœ¬èº«å¤æ‚æ•°æ®çš„å­˜è´®ã€‚ä½†æ˜¯æŒä¹…åŒ–åçš„å­—ä¸²æ˜¯ä¸å¯è®¤è¯»çš„ï¼Œå¹¶ä¸”åªèƒ½ç”¨äºPythonç¯å¢ƒï¼Œä¸èƒ½ç”¨ä½œä¸å…¶å®ƒè¯­è¨€è¿›è¡Œæ•°æ®äº¤æ¢ã€‚
>
> æŠŠ Python å¯¹è±¡ç›´æ¥ä¿å­˜åˆ°æ–‡ä»¶é‡Œï¼Œè€Œä¸éœ€è¦å…ˆæŠŠå®ƒä»¬è½¬åŒ–ä¸ºå­—ç¬¦ä¸²å†ä¿å­˜ï¼Œä¹Ÿä¸éœ€è¦ç”¨åº•å±‚çš„æ–‡ä»¶è®¿é—®æ“ä½œï¼Œç›´æ¥æŠŠå®ƒä»¬å†™å…¥åˆ°ä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶é‡Œã€‚pickle æ¨¡å—ä¼šåˆ›å»ºä¸€ä¸ª Python è¯­è¨€ä¸“ç”¨çš„äºŒè¿›åˆ¶æ ¼å¼ï¼Œä¸éœ€è¦ä½¿ç”¨è€…è€ƒè™‘ä»»ä½•æ–‡ä»¶ç»†èŠ‚ï¼Œå®ƒä¼šå¸®ä½ å®Œæˆè¯»å†™å¯¹è±¡æ“ä½œã€‚ç”¨pickleæ¯”ä½ æ‰“å¼€æ–‡ä»¶ã€è½¬æ¢æ•°æ®æ ¼å¼å¹¶å†™å…¥è¿™æ ·çš„æ“ä½œè¦èŠ‚çœä¸å°‘ä»£ç è¡Œã€‚

â€‹	æˆ‘ä»¬ä¸‹è½½çš„æ˜¯å‹ç¼©çš„æ–‡ä»¶åŒ…ï¼ˆå—¯ï¼Œæ²¡äººå–œæ¬¢æŠŠæ•°æ®æ’’æ‹‰å‡ºæ¥ä¸€å¤§å †ï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸€ä¸ªå‹ç¼©åŒ…åº“æ¥è¯»å–é‡Œå¤´çš„æ•°æ®ï¼š

```
import pickle
import gzip

with gzip.open((PATH / FILENAME).as_posix(), "rb") as f:
        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding="latin-1")
```

>å…³äºgzipçš„ä½¿ç”¨ï¼š[pythonä¸­gzipæ¨¡å—çš„ä½¿ç”¨ - eliwang - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/eliwang/p/14591861.html)

### åŠ è½½æ•°æ®é›†

```
from matplotlib import pyplot
import numpy as np

print("The Mnist Train DataSet's size is {0}".format(len(x_train)))
pyplot.imshow(x_train[2].reshape((28, 28)), cmap="gray")
print(x_train.shape)
```

â€‹	æˆ‘ä»¬ä¸Šé¢çš„ä¸¤è¡Œä»£ç å°†x, yä¸¤ä¸ªè®­ç»ƒé›†çš„æ•°æ®é›†åŠ è½½åˆ°äº†ç¨‹åºé‡Œå¤´ã€‚

![image-20240421184729553](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240421184729553.png)

â€‹	ç°åœ¨ï¼Œæˆ‘ä»¬ç»ˆäºå¯ä»¥æŠŠç›®å…‰æ”¾åˆ°ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å¤„ç†è¿™äº›æ•°æ®äº†ï¼š

â€‹	å®é™…ä¸Šæµç¨‹å›¾æ˜¯è¿™æ ·çš„ï¼šè¿™ä¸ªç¥ç»ç½‘ç»œå­¦ä¹ è¿™äº›åƒç´ çš„æ’å¸ƒä»¥è®¤è¯†è¿™äº›æ•°å­—å¯¹åº”çš„åƒç´ æ’å¸ƒæ˜¯å¦‚ä½•çš„ï¼Œåè¿‡æ¥åšï¼Œæˆ‘ä»¬å°±å¯ä»¥æ ¹æ®ä¸€ä¸ªå›¾åƒå¾—åˆ°è¿™ä¸ªæ•°å­—æ˜¯ä»€ä¹ˆã€‚

![5](D:\My notebook project\Exports\é…å¥—èµ„æ–™ï¼ˆè®²ä¹‰ã€æºç ï¼‰\ç¬¬äºŒç« ï¼šç¥ç»ç½‘ç»œå®æˆ˜åˆ†ç±»ä¸å›å½’ä»»åŠ¡\ç¥ç»ç½‘ç»œå®æˆ˜åˆ†ç±»ä¸å›å½’ä»»åŠ¡\img\5.png)![4](D:\My notebook project\Exports\é…å¥—èµ„æ–™ï¼ˆè®²ä¹‰ã€æºç ï¼‰\ç¬¬äºŒç« ï¼šç¥ç»ç½‘ç»œå®æˆ˜åˆ†ç±»ä¸å›å½’ä»»åŠ¡\ç¥ç»ç½‘ç»œå®æˆ˜åˆ†ç±»ä¸å›å½’ä»»åŠ¡\img\4.png)

â€‹	å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæ¦‚ç‡å›¾è°±ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼šä»–æ˜¯ä½•ç§æ•°å­—çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼‰

â€‹	æ‰€ä»¥ï¼Œä¸ºäº†è½¬åŒ–æˆæ¨¡å‹å¯ä»¥æ¥å—çš„æ•°æ®æ ¼å¼ï¼Œéœ€è¦å°†ä¹‹è½¬åŒ–ä¸ºTensor:

```python
import torch

x_train, y_train, x_valid, y_valid = map(
    torch.tensor, (x_train, y_train, x_valid, y_valid)
)

# ä¸‹é¢æ˜¯æ‰“å°æ£€æŸ¥ï¼Œå¯ä»¥ä¸éœ€è¦
n, c = x_train.shape
print("n= {0}, c={1}".format(n, c))
x_train, x_train.shape, y_train.min(), y_train.max()
print(x_train, y_train)
print(x_train.shape)
print(y_train.min(), y_train.max())
```

```
n= 50000, c=784
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])
torch.Size([50000, 784])
tensor(0) tensor(9)
```

â€‹	è¿™å°±æ˜¯æ‰“å°å‡ºæ¥çš„ç»“æœã€‚

### å®šä¹‰æŸå¤±å‡½æ•°

â€‹	**æŸå¤±å‡½æ•°ï¼Œåˆå«ç›®æ ‡å‡½æ•°ï¼Œç”¨äºè®¡ç®—çœŸå®å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´å·®å¼‚çš„å‡½æ•°ï¼Œå’Œä¼˜åŒ–å™¨æ˜¯ç¼–è¯‘ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹çš„é‡è¦è¦ç´ **ã€‚ æŸå¤±Losså¿…é¡»æ˜¯æ ‡é‡ï¼Œå› ä¸ºå‘é‡æ— æ³•æ¯”è¾ƒå¤§å°ï¼ˆå‘é‡æœ¬èº«éœ€è¦é€šè¿‡èŒƒæ•°ç­‰æ ‡é‡æ¥æ¯”è¾ƒï¼‰ã€‚ æŸå¤±å‡½æ•°ä¸€èˆ¬åˆ†ä¸º4ç§ï¼ŒHingeLoss 0-1 æŸå¤±å‡½æ•°ï¼Œç»å¯¹å€¼æŸå¤±å‡½æ•°ï¼Œå¹³æ–¹æŸå¤±å‡½æ•°ï¼Œå¯¹æ•°æŸå¤±å‡½æ•°ã€‚

â€‹	ä»»ä½•ä¸€ä¸ªæœ‰**è´Ÿå¯¹æ•°ä¼¼ç„¶ç»„æˆçš„æŸå¤±**éƒ½æ˜¯å®šä¹‰åœ¨è®­ç»ƒé›†ä¸Šçš„ç»éªŒåˆ†å¸ƒå’Œå®šä¹‰åœ¨æ¨¡å‹ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„äº¤å‰ç†µã€‚ä¾‹å¦‚ï¼Œå‡æ–¹è¯¯å·®æ˜¯ç»éªŒåˆ†å¸ƒå’Œé«˜æ–¯æ¨¡å‹ä¹‹é—´çš„äº¤å‰ç†µ

â€‹	è¿™é‡Œæˆ‘ä»¬ä¸¤ç§å®è·µæ–¹æ¡ˆï¼šå°±æ˜¯ä½¿ç”¨`torch.nn.functional`æˆ–è€…æ˜¯`torch.nn.Module`ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¦‚æœæ¨¡å‹æœ‰å¯å­¦ä¹ çš„å‚æ•°ï¼Œæœ€å¥½ç”¨nn.Moduleï¼Œå…¶ä»–æƒ…å†µnn.functionalç›¸å¯¹æ›´ç®€å•ä¸€äº›ã€‚

â€‹	æ‰€ä»¥è¿™é‡Œï¼Œæˆ‘ä»¬åªæ˜¯å®Œæˆäº¤å‰ç†µæŸå¤±è®¡ç®—ï¼Œæ²¡å¿…è¦ä¸Šå¤§ç‚®ï¼š

```
import torch.nn.functional
loss_func = torch.nn.functional.cross_entropy
def model(xb):
    return xb.mm(weights) + bias
```

```python
bs = 64
xb = x_train[0:bs]  # a mini-batch from x
yb = y_train[0:bs]
# ç”Ÿæˆä¸€ç»„éšæœºçš„æƒé‡
weights = torch.randn([784, 10], dtype = torch.float,  requires_grad = True) 
bs = 64
bias = torch.zeros(10, requires_grad=True)

print(loss_func(model(xb), yb))
```

â€‹	çœ‹çœ‹æˆ‘ä»¬ä½¿ç”¨loss_funcçš„è¡Œä¸ºå¦‚ä½•

```python
tensor(10.8561, grad_fn=<NllLossBackward0>)
```

### åˆ›å»ºæ¨¡å‹

â€‹	æˆ‘ä»¬ç°åœ¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ¨¡å‹äº†ï¼š

â€‹	å€¼å¾—æ³¨æ„çš„æ˜¯ï¼š

>å¿…é¡»ç»§æ‰¿nn.Moduleä¸”åœ¨å…¶æ„é€ å‡½æ•°ä¸­éœ€è°ƒç”¨nn.Moduleçš„æ„é€ å‡½æ•°
>
>æ— éœ€å†™åå‘ä¼ æ’­å‡½æ•°ï¼Œnn.Moduleèƒ½å¤Ÿåˆ©ç”¨autogradè‡ªåŠ¨å®ç°åå‘ä¼ æ’­
>
>Moduleä¸­çš„å¯å­¦ä¹ å‚æ•°å¯ä»¥é€šè¿‡named_parameters()æˆ–è€…parameters()è¿”å›è¿­ä»£å™¨

```
from torch import nn

class Mnist_NN(nn.Module):
    def __init__(self):
    	# æ’å¸ƒå¥½å±‚
        super().__init__()
        self.hidden1 = nn.Linear(784, 128) # è¿™é‡Œçš„å‚æ•°è¯´çš„å°±æ˜¯è¾“å…¥å¦‚ä½•ï¼Œè¾“å‡ºå¦‚ä½• 748 -> 128 å›¾åƒå¤§å°748
        self.hidden2 = nn.Linear(128, 256) # 128 -> 256
        self.out  = nn.Linear(256, 10)	# 256 -> 10 ç»“æœæ˜¯10ä¸ª

    def forward(self, x):
    	# å‘å‰æ¨ç†çš„å‡½æ•°
        x = F.relu(self.hidden1(x))
        x = F.relu(self.hidden2(x))
        x = self.out(x)
        return x
```

```
net = Mnist_NN() # å®ä¾‹åŒ–ä¸€ä¸ªç½‘ç»œå‡ºæ¥
print(net)
```

â€‹	å¯ä»¥çœ‹çœ‹ç»“æœï¼š

```
hidden1.weight Parameter containing:
tensor([[ 0.0352,  0.0222,  0.0229,  ..., -0.0060, -0.0189, -0.0097],
        [-0.0067,  0.0222, -0.0206,  ..., -0.0217,  0.0225,  0.0059],
        [ 0.0255, -0.0330,  0.0099,  ...,  0.0030, -0.0336, -0.0115],
        ...,
        [ 0.0060,  0.0357,  0.0064,  ..., -0.0156,  0.0180,  0.0014],
        [-0.0235,  0.0185,  0.0324,  ...,  0.0238,  0.0045, -0.0037],
        [ 0.0342,  0.0153,  0.0076,  ..., -0.0325, -0.0033, -0.0302]],
       requires_grad=True) torch.Size([128, 784])
hidden1.bias Parameter containing:
tensor([ 0.0089, -0.0122, -0.0346,  0.0320, -0.0041, -0.0223, -0.0190,  0.0334,
        -0.0238,  0.0318, -0.0080, -0.0203,  0.0054,  0.0291, -0.0009, -0.0321,
         0.0285,  0.0037,  0.0058, -0.0081, -0.0128,  0.0249,  0.0216, -0.0272,
         0.0097,  0.0126, -0.0058, -0.0149,  0.0192,  0.0115, -0.0009, -0.0009,
         0.0065, -0.0151,  0.0241,  0.0203,  0.0239, -0.0265, -0.0227,  0.0166,
         0.0351, -0.0198, -0.0105, -0.0181,  0.0059, -0.0214,  0.0196,  0.0210,
        -0.0053, -0.0342, -0.0183, -0.0230, -0.0006, -0.0349,  0.0306,  0.0018,
        -0.0130,  0.0113,  0.0059, -0.0110,  0.0043,  0.0049,  0.0171,  0.0256,
        -0.0287, -0.0170, -0.0231,  0.0072, -0.0046, -0.0164,  0.0252, -0.0138,
        -0.0206, -0.0213, -0.0110, -0.0056, -0.0336,  0.0229, -0.0170, -0.0048,
         0.0185, -0.0346,  0.0248,  0.0061, -0.0257,  0.0040, -0.0019, -0.0145,
        -0.0328,  0.0062,  0.0053, -0.0172, -0.0061, -0.0245, -0.0230,  0.0222,
         0.0216,  0.0206,  0.0062, -0.0151, -0.0266,  0.0320, -0.0082,  0.0160,
         0.0224,  0.0243,  0.0039,  0.0010,  0.0316,  0.0178,  0.0290,  0.0259,
         0.0215, -0.0204, -0.0100,  0.0303,  0.0062,  0.0035,  0.0245, -0.0189,
...
       requires_grad=True) torch.Size([10, 256])
out.bias Parameter containing:
tensor([-0.0375,  0.0110, -0.0267, -0.0310,  0.0062,  0.0310,  0.0617, -0.0109,
         0.0520,  0.0088], requires_grad=True) torch.Size([10])
```

### ä½¿ç”¨DataSetå’ŒDataLoaderåŒ–ç®€æˆ‘ä»¬çš„æ•°æ®å°è£…

```
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader

train_ds = TensorDataset(x_train, y_train)
train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)

valid_ds = TensorDataset(x_valid, y_valid)
valid_dl = DataLoader(valid_ds, batch_size=bs * 2)
def get_data(train_ds, valid_ds, bs):
    return (
        DataLoader(train_ds, batch_size=bs, shuffle=True),
        DataLoader(valid_ds, batch_size=bs * 2),
    )
```

â€‹	æˆ‘ä»¬å°†æ•°æ®å°è£…æˆç¨åå‡½æ•°å°†ä¼šç”¨åˆ°çš„æ ¼å¼ï¼Œå¦‚ä¸ŠğŸ‘†

```
import numpy as np
# ä¸‹é¢å®šä¹‰æ‹Ÿåˆå‡½æ•°ï¼Œè¿™é‡Œå®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬çš„è®­ç»ƒåº”ç”¨å±‚ï¼
# Steps: è®­ç»ƒçš„è®ºæ•°
# loss_func: æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„æŸå¤±å‡½æ•°
# opt: è¿™é‡Œæˆ‘ä»¬æŒ‡ä»£çš„æ˜¯ä¼˜åŒ–ç®—å­æ–¹æ³•ï¼Œæ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬è¿™é‡Œä½¿ç”¨SGD
# train_dl: è®­ç»ƒçš„æ•°æ®é›†
# valid_dl: éªŒè¯çš„æ•°æ®é›†
def fit(steps, model, loss_func, opt, train_dl, valid_dl):
    for step in range(steps):
        model.train()
        for xb, yb in train_dl:
            loss_batch(model, loss_func, xb, yb, opt)

        model.eval()
        with torch.no_grad():
            losses, nums = zip(
                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]
            )
        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)
        print('å½“å‰step:'+str(step), 'éªŒè¯é›†æŸå¤±ï¼š'+str(val_loss))
```



>ä¸€èˆ¬åœ¨è®­ç»ƒæ¨¡å‹æ—¶åŠ ä¸Šmodel.train()ï¼Œè¿™æ ·ä¼šæ­£å¸¸ä½¿ç”¨Batch Normalizationå’Œ Dropout
>
>æµ‹è¯•çš„æ—¶å€™ä¸€èˆ¬é€‰æ‹©model.eval()ï¼Œè¿™æ ·å°±ä¸ä¼šä½¿ç”¨Batch Normalizationå’Œ Dropout

â€‹	ä¸‹é¢ï¼Œæˆ‘ä»¬ç»ˆäºå¯ä»¥å¼€å§‹è¿›è¡Œæ•°æ®é›†è®­ç»ƒäº†ï¼š

```
train_dl, valid_dl = get_data(train_ds, valid_ds, bs)
model, opt = get_model()
fit(30, model, loss_func, opt, train_dl, valid_dl)
```

â€‹	çœ‹çœ‹ç»“æœï¼

```
å½“å‰step:0 éªŒè¯é›†æŸå¤±ï¼š2.273305953979492
å½“å‰step:1 éªŒè¯é›†æŸå¤±ï¼š2.237687868499756
å½“å‰step:2 éªŒè¯é›†æŸå¤±ï¼š2.1822078701019287
å½“å‰step:3 éªŒè¯é›†æŸå¤±ï¼š2.0902939651489256
å½“å‰step:4 éªŒè¯é›†æŸå¤±ï¼š1.9395794193267821
å½“å‰step:5 éªŒè¯é›†æŸå¤±ï¼š1.7149433799743652
å½“å‰step:6 éªŒè¯é›†æŸå¤±ï¼š1.4407895727157594
å½“å‰step:7 éªŒè¯é›†æŸå¤±ï¼š1.1795400686264037
å½“å‰step:8 éªŒè¯é›†æŸå¤±ï¼š0.9769629907608032
å½“å‰step:9 éªŒè¯é›†æŸå¤±ï¼š0.8321391986846924
å½“å‰step:10 éªŒè¯é›†æŸå¤±ï¼š0.7286654480934143
å½“å‰step:11 éªŒè¯é›†æŸå¤±ï¼š0.6524578206062317
å½“å‰step:12 éªŒè¯é›†æŸå¤±ï¼š0.5956221778869629
å½“å‰step:13 éªŒè¯é›†æŸå¤±ï¼š0.5517672945976257
å½“å‰step:14 éªŒè¯é›†æŸå¤±ï¼š0.5173421884059906
å½“å‰step:15 éªŒè¯é›†æŸå¤±ï¼š0.48939590430259705
å½“å‰step:16 éªŒè¯é›†æŸå¤±ï¼š0.4668446585178375
å½“å‰step:17 éªŒè¯é›†æŸå¤±ï¼š0.4478798774242401
å½“å‰step:18 éªŒè¯é›†æŸå¤±ï¼š0.43193086733818054
å½“å‰step:19 éªŒè¯é›†æŸå¤±ï¼š0.4181952142238617
å½“å‰step:20 éªŒè¯é›†æŸå¤±ï¼š0.40629616613388064
å½“å‰step:21 éªŒè¯é›†æŸå¤±ï¼š0.396016849565506
å½“å‰step:22 éªŒè¯é›†æŸå¤±ï¼š0.38692939014434813
å½“å‰step:23 éªŒè¯é›†æŸå¤±ï¼š0.3788792317867279
å½“å‰step:24 éªŒè¯é›†æŸå¤±ï¼š0.3714227032661438
å½“å‰step:25 éªŒè¯é›†æŸå¤±ï¼š0.36518091886043547
å½“å‰step:26 éªŒè¯é›†æŸå¤±ï¼š0.3593594216585159
å½“å‰step:27 éªŒè¯é›†æŸå¤±ï¼š0.3533795116186142
å½“å‰step:28 éªŒè¯é›†æŸå¤±ï¼š0.34808647682666777
å½“å‰step:29 éªŒè¯é›†æŸå¤±ï¼š0.3436836592435837
```

## ä½¿ç”¨Pytorchç¥ç»ç½‘ç»œè¿›è¡Œæ°”æ¸©é¢„æµ‹

### ä½¿ç”¨åˆ°çš„åŒ…ï¼š

```
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import warnings
warnings.filterwarnings("ignore")
%matplotlib inline
```

```
features = pd.read_csv('temps.csv')

#çœ‹çœ‹æ•°æ®é•¿ä»€ä¹ˆæ ·å­
features.head()
# å¦‚æœæƒ³è¦è°ƒæ•´ï¼Œå¯ä»¥è¾“å…¥ä¸€ä¸ªæ•°ç»„æŒ‡å®šæŸ¥çœ‹è‹¥å¹²è¡Œ
```

![image-20240421191603674](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240421191603674.png)

```
æ•°æ®è¡¨ä¸­
* year,moth,day,weekåˆ†åˆ«è¡¨ç¤ºçš„å…·ä½“çš„æ—¶é—´
* temp_2ï¼šå‰å¤©çš„æœ€é«˜æ¸©åº¦å€¼
* temp_1ï¼šæ˜¨å¤©çš„æœ€é«˜æ¸©åº¦å€¼
* averageï¼šåœ¨å†å²ä¸­ï¼Œæ¯å¹´è¿™ä¸€å¤©çš„å¹³å‡æœ€é«˜æ¸©åº¦å€¼
* actualï¼šè¿™å°±æ˜¯æˆ‘ä»¬çš„æ ‡ç­¾å€¼äº†ï¼Œå½“å¤©çš„çœŸå®æœ€é«˜æ¸©åº¦
* friendï¼šè¿™ä¸€åˆ—å¯èƒ½æ˜¯å‡‘çƒ­é—¹çš„ï¼Œä½ çš„æœ‹å‹çŒœæµ‹çš„å¯èƒ½å€¼ï¼Œå’±ä»¬ä¸ç®¡å®ƒå°±å¥½äº†
```

> æˆ‘æƒ³è¯´çš„æ˜¯ï¼šè¿™è·Ÿæˆ‘ä»¬å®é™…æ”¶é›†åˆ°çš„æ•°æ®ä¸€æ ·ï¼Œå¯èƒ½ä¼šå­˜åœ¨ä¸€äº›åƒåœ¾å€¼éœ€è¦æˆ‘ä»¬çš„å¤„ç†ï¼

```
# å¤„ç†æ—¶é—´æ•°æ®
import datetime

# åˆ†åˆ«å¾—åˆ°å¹´ï¼Œæœˆï¼Œæ—¥
years = features['year']
months = features['month']
days = features['day']

# datetimeæ ¼å¼
dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]
dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]
dates[:5]
```

```
[datetime.datetime(2016, 1, 1, 0, 0),
 datetime.datetime(2016, 1, 2, 0, 0),
 datetime.datetime(2016, 1, 3, 0, 0),
 datetime.datetime(2016, 1, 4, 0, 0),
 datetime.datetime(2016, 1, 5, 0, 0)]
```

â€‹	æˆ‘ä»¬æ¥ç”»å›¾çœ‹çœ‹ï¼š

```
# å‡†å¤‡ç”»å›¾
# æŒ‡å®šé»˜è®¤é£æ ¼
plt.style.use('fivethirtyeight')

# è®¾ç½®å¸ƒå±€
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (10,10))
fig.autofmt_xdate(rotation = 45)

# æ ‡ç­¾å€¼
ax1.plot(dates, features['actual'])
ax1.set_xlabel(''); ax1.set_ylabel('Temperature'); ax1.set_title('Max Temp')

# æ˜¨å¤©
ax2.plot(dates, features['temp_1'])
ax2.set_xlabel(''); ax2.set_ylabel('Temperature'); ax2.set_title('Previous Max Temp')

# å‰å¤©
ax3.plot(dates, features['temp_2'])
ax3.set_xlabel('Date'); ax3.set_ylabel('Temperature'); ax3.set_title('Two Days Prior Max Temp')

# æˆ‘çš„é€—é€¼æœ‹å‹
ax4.plot(dates, features['friend'])
ax4.set_xlabel('Date'); ax4.set_ylabel('Temperature'); ax4.set_title('Friend Estimate')

plt.tight_layout(pad=2)
```

â€‹	ä¸‹é¢ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¤„ç†æ•°æ®çš„æ ‡ç­¾é—®é¢˜ï¼šå¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„å±æ€§è¿˜æ˜¯å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬å¦‚ä½•å°†ä¹‹æ˜ å°„ä¸ºç½‘ç»œå¯ä»¥ç†è§£çš„ç»“æ„å‘¢ï¼šä½¿ç”¨ç‹¬çƒ­ç¼–ç ï¼š

>åœ¨å¾ˆå¤šæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œç‰¹å¾å¹¶ä¸æ€»æ˜¯è¿ç»­å€¼ï¼Œè€Œæœ‰å¯èƒ½æ˜¯åˆ†ç±»å€¼ã€‚

ç¦»æ•£ç‰¹å¾çš„ç¼–ç åˆ†ä¸ºä¸¤ç§æƒ…å†µï¼š

1. ç¦»æ•£ç‰¹å¾çš„å–å€¼ä¹‹é—´æ²¡æœ‰å¤§å°çš„æ„ä¹‰ï¼Œæ¯”å¦‚colorï¼š[red,blue],é‚£ä¹ˆå°±ä½¿ç”¨one-hotç¼–ç 
2. ç¦»æ•£ç‰¹å¾çš„å–å€¼æœ‰å¤§å°çš„æ„ä¹‰ï¼Œæ¯”å¦‚size:[X,XL,XXL],é‚£ä¹ˆå°±ä½¿ç”¨æ•°å€¼çš„æ˜ å°„{X:1,XL:2,XXL:3}

```
# æ ‡ç­¾
labels = np.array(features['actual'])

# åœ¨ç‰¹å¾ä¸­å»æ‰æ ‡ç­¾
features= features.drop('actual', axis = 1)

# åå­—å•ç‹¬ä¿å­˜ä¸€ä¸‹ï¼Œä»¥å¤‡åæ‚£
feature_list = list(features.columns)

# è½¬æ¢æˆåˆé€‚çš„æ ¼å¼
features = np.array(features)
```

â€‹	è¿™ä¸ªæ—¶å€™æˆ‘ä»¬ä¼šå‘ç°ï¼Œè¿™äº›ç‰¹å¾çš„é‡çº§å¤§å°ä¸ä¸€ï¼Œä¼šå¾ˆå®¹æ˜“é€ æˆåç½®ï¼Œä¸€ä¸ªåŠæ³•å°±æ˜¯å¯¹ä¹‹è¿›è¡Œæ­£åˆ™åŒ–ï¼š

```
from sklearn import preprocessing
# ä½¿ç”¨æ ‡å‡†åŒ–æ¥å¤„ç†æ•°æ®
# fitåå˜åŒ–
input_features = preprocessing.StandardScaler().fit_transform(features)
```

```
# çœ‹çœ‹ç»“æœï¼š
input_features[0]
array([ 0.        , -1.5678393 , -1.65682171, -1.48452388, -1.49443549,
       -1.3470703 , -1.98891668,  2.44131112, -0.40482045, -0.40961596,
       -0.40482045, -0.40482045, -0.41913682, -0.40482045])
```

### æ„å»ºç½‘ç»œæ¨¡å‹

â€‹	ä¸¤ç§æ–¹å¼ï¼šä¸€ç§æˆ‘ä»¬æ‰‹åŠ¨æ¥ï¼š

```
x = torch.tensor(input_features, dtype = float)
y = torch.tensor(labels, dtype = float)

# æƒé‡å‚æ•°åˆå§‹åŒ–
weights = torch.randn((14, 128), dtype = float, requires_grad = True) 
biases = torch.randn(128, dtype = float, requires_grad = True) 
weights2 = torch.randn((128, 1), dtype = float, requires_grad = True) 
biases2 = torch.randn(1, dtype = float, requires_grad = True) 

learning_rate = 0.001 
losses = []

for i in range(1000):
    # è®¡ç®—éšå±‚
    hidden = x.mm(weights) + biases
    # åŠ å…¥æ¿€æ´»å‡½æ•°
    hidden = torch.relu(hidden)
    # é¢„æµ‹ç»“æœ
    predictions = hidden.mm(weights2) + biases2
    # é€šè®¡ç®—æŸå¤±
    loss = torch.mean((predictions - y) ** 2) 
    losses.append(loss.data.numpy())
    
    # æ‰“å°æŸå¤±å€¼
    if i % 100 == 0:
        print('loss:', loss)
    #è¿”å‘ä¼ æ’­è®¡ç®—
    loss.backward()
    
    #æ›´æ–°å‚æ•°
    weights.data.add_(- learning_rate * weights.grad.data)  
    biases.data.add_(- learning_rate * biases.grad.data)
    weights2.data.add_(- learning_rate * weights2.grad.data)
    biases2.data.add_(- learning_rate * biases2.grad.data)
    
    # æ¯æ¬¡è¿­ä»£éƒ½å¾—è®°å¾—æ¸…ç©º
    weights.grad.data.zero_()
    biases.grad.data.zero_()
    weights2.grad.data.zero_()
    biases2.grad.data.zero_()
```

```
loss: tensor(4447.1342, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(157.0553, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(148.6876, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(145.6146, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(143.9219, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(142.8372, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(142.0914, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(141.5533, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(141.1492, dtype=torch.float64, grad_fn=<MeanBackward0>)
loss: tensor(140.8404, dtype=torch.float64, grad_fn=<MeanBackward0>)
```

â€‹	å¦ä¸€ç§ç”¨åº“ï¼š

```
input_size = input_features.shape[1]
hidden_size = 128
output_size = 1
batch_size = 16
my_nn = torch.nn.Sequential(
    torch.nn.Linear(input_size, hidden_size),
    torch.nn.Sigmoid(),
    torch.nn.Linear(hidden_size, output_size),
)
cost = torch.nn.MSELoss(reduction='mean')
optimizer = torch.optim.Adam(my_nn.parameters(), lr = 0.001)
# è®­ç»ƒç½‘ç»œ
losses = []
for i in range(1000):
    batch_loss = []
    # MINI-Batchæ–¹æ³•æ¥è¿›è¡Œè®­ç»ƒ
    for start in range(0, len(input_features), batch_size):
        end = start + batch_size if start + batch_size < len(input_features) else len(input_features)
        xx = torch.tensor(input_features[start:end], dtype = torch.float, requires_grad = True)
        yy = torch.tensor(labels[start:end], dtype = torch.float, requires_grad = True)
        prediction = my_nn(xx)
        loss = cost(prediction, yy)
        optimizer.zero_grad()
        loss.backward(retain_graph=True)
        optimizer.step()
        batch_loss.append(loss.data.numpy())
    
    # æ‰“å°æŸå¤±
    if i % 100==0:
        losses.append(np.mean(batch_loss))
        print(i, np.mean(batch_loss))
```

â€‹	æˆ‘ä»¬å–å‡ºç»“æœï¼š

```
x = torch.tensor(input_features, dtype = torch.float)
predict = my_nn(x).data.numpy()
```

â€‹	å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šæˆ‘ä»¬çš„éªŒè¯é›†ä¹Ÿéœ€è¦æŒ‰ç…§ç›¸åŒçš„æ–¹å¼è¿›è¡Œé¢„å¤„ç†åæ‰å¯ä»¥æ”¾åˆ°æ¨¡å‹é‡Œé¢„æµ‹ï¼š

```
# è½¬æ¢æ—¥æœŸæ ¼å¼
dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]
dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]

# åˆ›å»ºä¸€ä¸ªè¡¨æ ¼æ¥å­˜æ—¥æœŸå’Œå…¶å¯¹åº”çš„æ ‡ç­¾æ•°å€¼
true_data = pd.DataFrame(data = {'date': dates, 'actual': labels})

# åŒç†ï¼Œå†åˆ›å»ºä¸€ä¸ªæ¥å­˜æ—¥æœŸå’Œå…¶å¯¹åº”çš„æ¨¡å‹é¢„æµ‹å€¼
months = features[:, feature_list.index('month')]
days = features[:, feature_list.index('day')]
years = features[:, feature_list.index('year')]

test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]

test_dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in test_dates]

predictions_data = pd.DataFrame(data = {'date': test_dates, 'prediction': predict.reshape(-1)}) 

# çœŸå®å€¼
plt.plot(true_data['date'], true_data['actual'], 'b-', label = 'actual')

# é¢„æµ‹å€¼
plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')
plt.xticks(rotation = '60'); 
plt.legend()

# å›¾å
plt.xlabel('Date'); plt.ylabel('Maximum Temperature (F)'); plt.title('Actual and Predicted Values');
```

![image-20240421192912926](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240421192912926.png)

## Reference

[Pythonç»˜å›¾é—®é¢˜ï¼šMatplotlibä¸­%matplotlib inlineæ˜¯ä»€ä¹ˆã€å¦‚ä½•ä½¿ç”¨ï¼Ÿ_%matplotlib inline mial-CSDNåšå®¢](https://blog.csdn.net/liangzuojiayi/article/details/78183783)

[ä¸€æ–‡å¸¦ä½ ææ‡‚Pythonä¸­pickleæ¨¡å— - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/419362785)

[pythonä¸­gzipæ¨¡å—çš„ä½¿ç”¨ - eliwang - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/eliwang/p/14591861.html)

[ç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/134495345)

# CNN

## ç†è®ºé¢„çƒ­

â€‹	è®¡ç®—æœºè§†è§‰å‘å±•åˆ°ä»Šå¤©ï¼Œç¦»ä¸å¼€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆå¤§ä¼™ç†ŸçŸ¥çš„CNNï¼‰ã€‚å·ç§¯ç¥ç»ç½‘ç»œè‡ªèº«çš„åº”ç”¨å°±ååˆ†çš„å¹¿æ³›ï¼šè¯¸å¦‚æ£€æµ‹ä»»åŠ¡ï¼Œåˆ†ç±»ï¼Œæ£€ç´¢ï¼Œè¶…åˆ†è¾¨ç‡é‡æ„ï¼ŒåŒ»å­¦ä»»åŠ¡ï¼Œæ— äººé©¾é©¶ï¼Œäººè„¸è¯†åˆ«ç­‰ã€‚

â€‹	å¯¹äºä¼ ç»Ÿç½‘ç»œï¼Œä»–çš„ä¸€ä¸ªé‡è¦åŒºåˆ«å°±æ˜¯å¤šäº†å·ç§¯å±‚å’Œæ± åŒ–å±‚ã€‚

![image-20240425192510482](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240425192510482.png)	æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„CNNçš„æ¶æ„å°±æ˜¯ï¼šè¾“å…¥å±‚ï¼Œå·ç§¯å±‚ï¼Œæ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚ã€‚é‡ç‚¹çœ‹å¤šå‡ºæ¥çš„ä¸¤ä¸ªå±‚ï¼š

â€‹	å·ç§¯å±‚é€šè¿‡å·ç§¯æ ¸å¯¹è¾“å…¥çš„å›¾åƒæ•°æ®è¿›è¡Œå·ç§¯æ“ä½œï¼Œä»¥æå–å›¾åƒçš„ç‰¹å¾ã€‚å·ç§¯æ ¸æ˜¯ä¸€ç§å°çš„ã€å¯å­¦ä¹ çš„çŸ©é˜µï¼Œå®ƒå¯ä»¥é€šè¿‡æ»‘åŠ¨å’Œæƒé‡æ¥å­¦ä¹ å›¾åƒçš„ç‰¹å¾ã€‚

>1. å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé€šé“åˆ†ç¦»ï¼Œå°†å…¶è½¬æ¢ä¸ºå¤šä¸ªé€šé“ã€‚
>2. å¯¹æ¯ä¸ªé€šé“è¿›è¡Œå·ç§¯æ“ä½œï¼Œä½¿ç”¨å·ç§¯æ ¸å¯¹è¾“å…¥å›¾åƒæ•°æ®è¿›è¡Œå·ç§¯ã€‚
>3. å¯¹å·ç§¯åçš„ç»“æœè¿›è¡Œæ¿€æ´»å‡½æ•°å¤„ç†ï¼Œå¦‚ReLUç­‰ã€‚
>4. æ»‘åŠ¨å·ç§¯æ ¸ä»¥è¦†ç›–æ•´ä¸ªè¾“å…¥å›¾åƒï¼Œå¹¶å°†å„ä¸ªå·ç§¯ç»“æœæ‹¼æ¥åœ¨ä¸€èµ·å½¢æˆæ–°çš„å›¾åƒã€‚
>5. å¯¹æ–°çš„å›¾åƒé‡å¤æ­¥éª¤2-4ï¼Œç›´åˆ°æ‰€æœ‰å·ç§¯å±‚çš„æ“ä½œå®Œæˆã€‚

â€‹	ä¸Šé¢çš„æ¦‚å¿µä¸­ï¼Œæˆ‘ä»¬å‘ç°å·ç§¯å±‚ä¸€å®šæ¶‰åŠåˆ°è¿™äº›å‚æ•°ï¼š

> æ»‘åŠ¨çª—å£æ­¥é•¿ï¼ˆä¸€æ¬¡å¹³ç§»å¤šå°‘å·ç§¯ï¼Ÿï¼‰
>
> å·ç§¯æ ¸å°ºå¯¸ï¼ˆè¿™ä¸ªå·ç§¯æ ¸å¤šå¤§ï¼Œæ“ä½œå¤šå¤§çš„çŸ©é˜µè¿›è¡Œå·ç§¯ï¼Ÿï¼‰
>
> è¾¹ç¼˜å¡«å……
>
> å·ç§¯æ ¸çš„ä¸ªæ•°

â€‹	æ± åŒ–å±‚é€šè¿‡é‡‡æ ·æ“ä½œå¯¹å·ç§¯å±‚çš„è¾“å‡ºè¿›è¡Œä¸‹é‡‡æ ·ï¼Œä»¥å‡å°‘å‚æ•°æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦çš„ç‰¹å¾ä¿¡æ¯ã€‚æ± åŒ–å±‚é€šå¸¸ä½¿ç”¨æœ€å¤§æ± åŒ–æˆ–å¹³å‡æ± åŒ–ä½œä¸ºé‡‡æ ·æ–¹æ³•ã€‚

>1. å¯¹è¾“å…¥å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œå°†å…¶åˆ’åˆ†ä¸ºå¤šä¸ªå°å—ã€‚
>2. å¯¹æ¯ä¸ªå°å—è¿›è¡Œé‡‡æ ·ï¼Œå¦‚é€‰æ‹©æœ€å¤§å€¼æˆ–å¹³å‡å€¼ç­‰ã€‚
>3. å¯¹é‡‡æ ·åçš„ç»“æœè¿›è¡Œä¸‹é‡‡æ ·ï¼Œä»¥å‡å°‘å›¾åƒçš„å¤§å°ã€‚
>4. æ»‘åŠ¨æ± åŒ–çª—å£ä»¥è¦†ç›–æ•´ä¸ªè¾“å…¥å›¾åƒï¼Œå¹¶å°†å„ä¸ªæ± åŒ–ç»“æœæ‹¼æ¥åœ¨ä¸€èµ·å½¢æˆæ–°çš„å›¾åƒã€‚
>5. å¯¹æ–°çš„å›¾åƒé‡å¤æ­¥éª¤2-4ï¼Œç›´åˆ°æ‰€æœ‰æ± åŒ–å±‚çš„æ“ä½œå®Œæˆã€‚

## å®è·µ

### æ„å»ºå·ç§¯ç¥ç»ç½‘ç»œ

- å·ç§¯ç½‘ç»œä¸­çš„è¾“å…¥å’Œå±‚ä¸ä¼ ç»Ÿç¥ç»ç½‘ç»œæœ‰äº›åŒºåˆ«ï¼Œéœ€é‡æ–°è®¾è®¡ï¼Œè®­ç»ƒæ¨¡å—åŸºæœ¬ä¸€è‡´

```
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets,transforms 
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
```

â€‹	è€è§„çŸ©è¯»æ•°æ®ï¼š

```python
# å®šä¹‰è¶…å‚æ•° 
input_size = 28  #å›¾åƒçš„æ€»å°ºå¯¸28*28
num_classes = 10  #æ ‡ç­¾çš„ç§ç±»æ•°
num_epochs = 3  #è®­ç»ƒçš„æ€»å¾ªç¯å‘¨æœŸ
batch_size = 64  #ä¸€ä¸ªæ’®ï¼ˆæ‰¹æ¬¡ï¼‰çš„å¤§å°ï¼Œ64å¼ å›¾ç‰‡

# è®­ç»ƒé›†
train_dataset = datasets.MNIST(root='./data',  
                            train=True,   
                            transform=transforms.ToTensor(),  
                            download=True) 

# æµ‹è¯•é›†
test_dataset = datasets.MNIST(root='./data', 
                           train=False, 
                           transform=transforms.ToTensor())

# æ„å»ºbatchæ•°æ®
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)
```

### å·ç§¯ç½‘ç»œæ¨¡å—æ„å»º

- ä¸€èˆ¬å·ç§¯å±‚ï¼Œreluå±‚ï¼Œæ± åŒ–å±‚å¯ä»¥å†™æˆä¸€ä¸ªå¥—é¤
- æ³¨æ„å·ç§¯æœ€åç»“æœè¿˜æ˜¯ä¸€ä¸ªç‰¹å¾å›¾ï¼Œéœ€è¦æŠŠå›¾è½¬æ¢æˆå‘é‡æ‰èƒ½åšåˆ†ç±»æˆ–è€…å›å½’ä»»åŠ¡

```python
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(         # è¾“å…¥å¤§å° (1, 28, 28)
            nn.Conv2d(
                in_channels=1,              # ç°åº¦å›¾
                out_channels=16,            # è¦å¾—åˆ°å‡ å¤šå°‘ä¸ªç‰¹å¾å›¾
                kernel_size=5,              # å·ç§¯æ ¸å¤§å°
                stride=1,                   # æ­¥é•¿
                padding=2,                  # å¦‚æœå¸Œæœ›å·ç§¯åå¤§å°è·ŸåŸæ¥ä¸€æ ·ï¼Œéœ€è¦è®¾ç½®padding=(kernel_size-1)/2 if stride=1
            ),                              # è¾“å‡ºçš„ç‰¹å¾å›¾ä¸º (16, 28, 28)
            nn.ReLU(),                      # reluå±‚
            nn.MaxPool2d(kernel_size=2),    # è¿›è¡Œæ± åŒ–æ“ä½œï¼ˆ2x2 åŒºåŸŸï¼‰, è¾“å‡ºç»“æœä¸ºï¼š (16, 14, 14)
        )
        self.conv2 = nn.Sequential(         # ä¸‹ä¸€ä¸ªå¥—é¤çš„è¾“å…¥ (16, 14, 14)
            nn.Conv2d(16, 32, 5, 1, 2),     # è¾“å‡º (32, 14, 14)
            nn.ReLU(),                      # reluå±‚
            nn.MaxPool2d(2),                # è¾“å‡º (32, 7, 7)
        )
        self.out = nn.Linear(32 * 7 * 7, 10)   # å…¨è¿æ¥å±‚å¾—åˆ°çš„ç»“æœ

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)           # flattenæ“ä½œï¼Œç»“æœä¸ºï¼š(batch_size, 32 * 7 * 7)
        output = self.out(x)
        return output
```

â€‹	æˆ‘ä»¬çš„è¯„ä¼°ä»¥å‡†ç¡®ç‡ä¸ºå‡†

```python
def accuracy(predictions, labels):
    pred = torch.max(predictions.data, 1)[1] 
    rights = pred.eq(labels.data.view_as(pred)).sum() 
    return rights, len(labels) 
```

â€‹	è®­ç»ƒï¼š

```python
# å®ä¾‹åŒ–
net = CNN() 
#æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss() 
#ä¼˜åŒ–å™¨
optimizer = optim.Adam(net.parameters(), lr=0.001) #å®šä¹‰ä¼˜åŒ–å™¨ï¼Œæ™®é€šçš„éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•

#å¼€å§‹è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    #å½“å‰epochçš„ç»“æœä¿å­˜ä¸‹æ¥
    train_rights = [] 
    
    for batch_idx, (data, target) in enumerate(train_loader):  #é’ˆå¯¹å®¹å™¨ä¸­çš„æ¯ä¸€ä¸ªæ‰¹è¿›è¡Œå¾ªç¯
        net.train()                             
        output = net(data) 
        loss = criterion(output, target) 
        optimizer.zero_grad() 
        loss.backward() 
        optimizer.step() 
        right = accuracy(output, target) 
        train_rights.append(right) 

    
        if batch_idx % 100 == 0: 
            
            net.eval() 
            val_rights = [] 
            
            for (data, target) in test_loader:
                output = net(data) 
                right = accuracy(output, target) 
                val_rights.append(right)
                
            #å‡†ç¡®ç‡è®¡ç®—
            train_r = (sum([tup[0] for tup in train_rights]), sum([tup[1] for tup in train_rights]))
            val_r = (sum([tup[0] for tup in val_rights]), sum([tup[1] for tup in val_rights]))

            print('å½“å‰epoch: {} [{}/{} ({:.0f}%)]\tæŸå¤±: {:.6f}\tè®­ç»ƒé›†å‡†ç¡®ç‡: {:.2f}%\tæµ‹è¯•é›†æ­£ç¡®ç‡: {:.2f}%'.format(
                epoch, batch_idx * batch_size, len(train_loader.dataset),
                100. * batch_idx / len(train_loader), 
                loss.data, 
                100. * train_r[0].numpy() / train_r[1], 
                100. * val_r[0].numpy() / val_r[1]))
```

```
å½“å‰epoch: 0 [0/60000 (0%)]	æŸå¤±: 2.313235	è®­ç»ƒé›†å‡†ç¡®ç‡: 7.81%	æµ‹è¯•é›†æ­£ç¡®ç‡: 16.60%
å½“å‰epoch: 0 [6400/60000 (11%)]	æŸå¤±: 0.286634	è®­ç»ƒé›†å‡†ç¡®ç‡: 75.54%	æµ‹è¯•é›†æ­£ç¡®ç‡: 91.12%
å½“å‰epoch: 0 [12800/60000 (21%)]	æŸå¤±: 0.171127	è®­ç»ƒé›†å‡†ç¡®ç‡: 84.37%	æµ‹è¯•é›†æ­£ç¡®ç‡: 94.98%
å½“å‰epoch: 0 [19200/60000 (32%)]	æŸå¤±: 0.140562	è®­ç»ƒé›†å‡†ç¡®ç‡: 88.05%	æµ‹è¯•é›†æ­£ç¡®ç‡: 95.41%
å½“å‰epoch: 0 [25600/60000 (43%)]	æŸå¤±: 0.116371	è®­ç»ƒé›†å‡†ç¡®ç‡: 90.04%	æµ‹è¯•é›†æ­£ç¡®ç‡: 96.92%
å½“å‰epoch: 0 [32000/60000 (53%)]	æŸå¤±: 0.079103	è®­ç»ƒé›†å‡†ç¡®ç‡: 91.50%	æµ‹è¯•é›†æ­£ç¡®ç‡: 97.56%
å½“å‰epoch: 0 [38400/60000 (64%)]	æŸå¤±: 0.139781	è®­ç»ƒé›†å‡†ç¡®ç‡: 92.45%	æµ‹è¯•é›†æ­£ç¡®ç‡: 97.73%
å½“å‰epoch: 0 [44800/60000 (75%)]	æŸå¤±: 0.029213	è®­ç»ƒé›†å‡†ç¡®ç‡: 93.12%	æµ‹è¯•é›†æ­£ç¡®ç‡: 97.96%
å½“å‰epoch: 0 [51200/60000 (85%)]	æŸå¤±: 0.023761	è®­ç»ƒé›†å‡†ç¡®ç‡: 93.71%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.13%
å½“å‰epoch: 0 [57600/60000 (96%)]	æŸå¤±: 0.073131	è®­ç»ƒé›†å‡†ç¡®ç‡: 94.17%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.01%
å½“å‰epoch: 1 [0/60000 (0%)]	æŸå¤±: 0.024543	è®­ç»ƒé›†å‡†ç¡®ç‡: 100.00%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.26%
å½“å‰epoch: 1 [6400/60000 (11%)]	æŸå¤±: 0.012003	è®­ç»ƒé›†å‡†ç¡®ç‡: 97.83%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.23%
å½“å‰epoch: 1 [12800/60000 (21%)]	æŸå¤±: 0.037428	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.22%	æµ‹è¯•é›†æ­£ç¡®ç‡: 97.90%
å½“å‰epoch: 1 [19200/60000 (32%)]	æŸå¤±: 0.039895	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.25%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.01%
å½“å‰epoch: 1 [25600/60000 (43%)]	æŸå¤±: 0.080754	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.20%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.10%
å½“å‰epoch: 1 [32000/60000 (53%)]	æŸå¤±: 0.190979	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.26%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.47%
å½“å‰epoch: 1 [38400/60000 (64%)]	æŸå¤±: 0.060385	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.28%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.62%
å½“å‰epoch: 1 [44800/60000 (75%)]	æŸå¤±: 0.024711	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.32%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.46%
å½“å‰epoch: 1 [51200/60000 (85%)]	æŸå¤±: 0.092447	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.37%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.57%
å½“å‰epoch: 1 [57600/60000 (96%)]	æŸå¤±: 0.089807	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.37%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.63%
å½“å‰epoch: 2 [0/60000 (0%)]	æŸå¤±: 0.040822	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.44%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.57%
å½“å‰epoch: 2 [6400/60000 (11%)]	æŸå¤±: 0.005734	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.76%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.46%
å½“å‰epoch: 2 [12800/60000 (21%)]	æŸå¤±: 0.104445	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.85%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.68%
å½“å‰epoch: 2 [19200/60000 (32%)]	æŸå¤±: 0.015682	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.79%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.88%
å½“å‰epoch: 2 [25600/60000 (43%)]	æŸå¤±: 0.012045	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.78%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.77%
å½“å‰epoch: 2 [32000/60000 (53%)]	æŸå¤±: 0.015652	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.85%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.45%
å½“å‰epoch: 2 [38400/60000 (64%)]	æŸå¤±: 0.040139	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.82%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.70%
å½“å‰epoch: 2 [44800/60000 (75%)]	æŸå¤±: 0.088626	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.80%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.81%
å½“å‰epoch: 2 [51200/60000 (85%)]	æŸå¤±: 0.007847	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.80%	æµ‹è¯•é›†æ­£ç¡®ç‡: 99.01%
å½“å‰epoch: 2 [57600/60000 (96%)]	æŸå¤±: 0.015996	è®­ç»ƒé›†å‡†ç¡®ç‡: 98.79%	æµ‹è¯•é›†æ­£ç¡®ç‡: 98.85%

```



## å®æˆ˜ï¼šåŸºäºç»å…¸ç½‘ç»œæ¶æ„è®­ç»ƒå›¾åƒåˆ†ç±»æ¨¡å‹

### æ•°æ®é¢„å¤„ç†éƒ¨åˆ†ï¼š

- æ•°æ®å¢å¼ºï¼štorchvisionä¸­transformsæ¨¡å—è‡ªå¸¦åŠŸèƒ½ï¼Œæ¯”è¾ƒå®ç”¨
- æ•°æ®é¢„å¤„ç†ï¼štorchvisionä¸­transformsä¹Ÿå¸®æˆ‘ä»¬å®ç°å¥½äº†ï¼Œç›´æ¥è°ƒç”¨å³å¯
- DataLoaderæ¨¡å—ç›´æ¥è¯»å–batchæ•°æ®

### ç½‘ç»œæ¨¡å—è®¾ç½®ï¼š

- **åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œtorchvisionä¸­æœ‰å¾ˆå¤šç»å…¸ç½‘ç»œæ¶æ„ï¼Œè°ƒç”¨èµ·æ¥ååˆ†æ–¹ä¾¿ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äººå®¶è®­ç»ƒå¥½çš„æƒé‡å‚æ•°æ¥ç»§ç»­è®­ç»ƒ**ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„è¿ç§»å­¦ä¹ 
- éœ€è¦æ³¨æ„çš„æ˜¯åˆ«äººè®­ç»ƒå¥½çš„ä»»åŠ¡è·Ÿå’±ä»¬çš„å¯ä¸æ˜¯å®Œå…¨ä¸€æ ·ï¼Œéœ€è¦æŠŠæœ€åçš„headå±‚æ”¹ä¸€æ”¹ï¼Œä¸€èˆ¬ä¹Ÿå°±æ˜¯æœ€åçš„å…¨è¿æ¥å±‚ï¼Œæ”¹æˆå’±ä»¬è‡ªå·±çš„ä»»åŠ¡
- è®­ç»ƒæ—¶å¯ä»¥å…¨éƒ¨é‡å¤´è®­ç»ƒï¼Œä¹Ÿå¯ä»¥åªè®­ç»ƒæœ€åå’±ä»¬ä»»åŠ¡çš„å±‚ï¼Œå› ä¸ºå‰å‡ å±‚éƒ½æ˜¯åšç‰¹å¾æå–çš„ï¼Œæœ¬è´¨ä»»åŠ¡ç›®æ ‡æ˜¯ä¸€è‡´çš„

### ç½‘ç»œæ¨¡å‹ä¿å­˜ä¸æµ‹è¯•

- æ¨¡å‹ä¿å­˜çš„æ—¶å€™å¯ä»¥å¸¦æœ‰é€‰æ‹©æ€§ï¼Œä¾‹å¦‚åœ¨éªŒè¯é›†ä¸­å¦‚æœå½“å‰æ•ˆæœå¥½åˆ™ä¿å­˜
- è¯»å–æ¨¡å‹è¿›è¡Œå®é™…æµ‹è¯•

![1](./Pytorchå­¦ä¹ ä¹‹è·¯/1-1714043789537-1.png)

## å®è·µ

â€‹	ç¬¬ä¸€æ­¥å°±æ˜¯å¯¼å…¥åº“ï¼š

```python
import os
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import torch
from torch import nn
import torch.optim as optim
import torchvision
# pip install torchvision
from torchvision import transforms, models, datasets
# https://pytorch.org/docs/stable/torchvision/index.html
import imageio
import time
import warnings
import random
import sys
import copy
import json
from PIL import Image
```

â€‹	ä¸‹é¢å°±æ˜¯åŠ è½½æ•°æ®é›†ï¼šå‡è®¾æˆ‘ä»¬çš„æ•°æ®é›†åœ°å€åœ¨

```python
data_dir = './flower_data/'
train_dir = data_dir + '/train' # è®­ç»ƒé›†æ•°æ®é›†
valid_dir = data_dir + '/valid' # éªŒè¯é›†æ•°æ®é›†
```

â€‹	ä»¥è¿™ä¸ªä¸ºä¾‹ï¼Œè‡ªè¡Œæ›´æ”¹åœ°å€ï¼š

### åˆ¶ä½œå¥½æ•°æ®æºï¼š

#### å›¾ç‰‡

â€‹	data_transformsä¸­æŒ‡å®šäº†æ‰€æœ‰å›¾åƒé¢„å¤„ç†æ“ä½œï¼ŒImageFolderå‡è®¾æ‰€æœ‰çš„æ–‡ä»¶æŒ‰æ–‡ä»¶å¤¹ä¿å­˜å¥½ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹ä¸‹é¢å­˜è´®åŒä¸€ç±»åˆ«çš„å›¾ç‰‡ï¼Œæ–‡ä»¶å¤¹çš„åå­—ä¸ºåˆ†ç±»çš„åå­—ã€‚

â€‹	é¢„å¤„ç†æ“ä½œå¦‚ä¸‹ï¼š

```python
data_transforms = {
    'train': transforms.Compose([transforms.RandomRotation(45),#éšæœºæ—‹è½¬ï¼Œ-45åˆ°45åº¦ä¹‹é—´éšæœºé€‰
        transforms.CenterCrop(224),#ä»ä¸­å¿ƒå¼€å§‹è£å‰ª
        transforms.RandomHorizontalFlip(p=0.5),#éšæœºæ°´å¹³ç¿»è½¬ é€‰æ‹©ä¸€ä¸ªæ¦‚ç‡æ¦‚ç‡
        transforms.RandomVerticalFlip(p=0.5),#éšæœºå‚ç›´ç¿»è½¬
        transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.1),#å‚æ•°1ä¸ºäº®åº¦ï¼Œå‚æ•°2ä¸ºå¯¹æ¯”åº¦ï¼Œå‚æ•°3ä¸ºé¥±å’Œåº¦ï¼Œå‚æ•°4ä¸ºè‰²ç›¸
        transforms.RandomGrayscale(p=0.025),#æ¦‚ç‡è½¬æ¢æˆç°åº¦ç‡ï¼Œ3é€šé“å°±æ˜¯R=G=B
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])#å‡å€¼ï¼Œæ ‡å‡†å·®
    ]),
    'valid': transforms.Compose([transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}
```

â€‹	ä¸‹é¢å°±æ˜¯ä½¿ç”¨ImageFolderæ‰¹é‡åŠ è½½å¯¼å…¥

```python
batch_size = 8

image_datasets = {
    x: datasets.ImageFolder(
        os.path.join(data_dir, x), data_transforms[x]
    ) for x in ['train', 'valid']
}

dataloaders = {
    x: torch.utils.data.DataLoader(
    image_datasets[x], 
    batch_size=batch_size, shuffle=True
    ) for x in ['train', 'valid']
}

dataset_sizes = {
    x: len(image_datasets[x]) for x in ['train', 'valid']
}

class_names = image_datasets['train'].classes
```

â€‹	æ¥çœ‹çœ‹ç»“æœï¼š

>image_datasets

```
{'train': Dataset ImageFolder
     Number of datapoints: 6552
     Root location: ./flower_data/train
     StandardTransform
 Transform: Compose(
                RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)
                CenterCrop(size=(224, 224))
                RandomHorizontalFlip(p=0.5)
                RandomVerticalFlip(p=0.5)
                ColorJitter(brightness=(0.8, 1.2), contrast=(0.9, 1.1), saturation=(0.9, 1.1), hue=(-0.1, 0.1))
                RandomGrayscale(p=0.025)
                ToTensor()
                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ),
 'valid': Dataset ImageFolder
     Number of datapoints: 818
     Root location: ./flower_data/valid
     StandardTransform
 Transform: Compose(
                Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)
                CenterCrop(size=(224, 224))
                ToTensor()
                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            )}
```

>dataloaders

```
{'train': <torch.utils.data.dataloader.DataLoader at 0x21f544caa30>,
 'valid': <torch.utils.data.dataloader.DataLoader at 0x21f544cb9d0>}
```

>dataset_sizes

```
{'train': 6552, 'valid': 818}
```

#### æ ‡ç­¾

â€‹	å¯¼å…¥æˆ‘ä»¬çš„æ ‡ç­¾æ–‡ä»¶

```python
with open('cat_to_name.json', 'r') as f:
    cat_to_name = json.load(f)
```

```
{'21': 'fire lily',
 '3': 'canterbury bells',
 '45': 'bolero deep blue',
 '1': 'pink primrose',
 '34': 'mexican aster',
 '27': 'prince of wales feathers',
 '7': 'moon orchid',
 '16': 'globe-flower',
 '25': 'grape hyacinth',
 '26': 'corn poppy',
 '79': 'toad lily',
 '39': 'siam tulip',
 '24': 'red ginger',
 '67': 'spring crocus',
 '35': 'alpine sea holly',
 '32': 'garden phlox',
 '10': 'globe thistle',
 '6': 'tiger lily',
 '93': 'ball moss',
 '33': 'love in the mist',
 '9': 'monkshood',
 '102': 'blackberry lily',
 '14': 'spear thistle',
 '19': 'balloon flower',
 '100': 'blanket flower',
...
 '89': 'watercress',
 '73': 'water lily',
 '46': 'wallflower',
 '77': 'passion flower',
 '51': 'petunia'}
```

â€‹	å¯ä»¥åœ¨æ§åˆ¶å°ä¸Šæ‰“å°ä¸€ä¸‹è¿™ä¸ªå˜é‡

### å±•ç¤ºä¸‹æ•°æ®

â€‹	æ³¨æ„tensorçš„æ•°æ®éœ€è¦è½¬æ¢æˆnumpyçš„æ ¼å¼ï¼Œè€Œä¸”è¿˜éœ€è¦è¿˜åŸå›æ ‡å‡†åŒ–çš„ç»“æœ

```python
def im_convert(tensor):
    """ å±•ç¤ºæ•°æ®"""
    
    image = tensor.to("cpu").clone().detach()
    image = image.numpy().squeeze()
    image = image.transpose(1,2,0)
    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
    image = image.clip(0, 1)

    return image
```

```python
fig=plt.figure(figsize=(20, 12))
columns = 4
rows = 2

dataiter = iter(dataloaders['valid'])
inputs, classes = dataiter.__next__()

for idx in range (columns*rows):
    ax = fig.add_subplot(rows, columns, idx+1, xticks=[], yticks=[])
    ax.set_title(cat_to_name[str(int(class_names[classes[idx]]))])
    plt.imshow(im_convert(inputs[idx]))
plt.show()
```

![image-20240425194012389](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240425194012389.png)

### åŠ è½½modelsä¸­æä¾›çš„æ¨¡å‹ï¼Œå¹¶ä¸”ç›´æ¥ç”¨è®­ç»ƒçš„å¥½æƒé‡å½“åšåˆå§‹åŒ–å‚æ•°

```python
model_name = 'resnet'  
#å¯é€‰çš„æ¯”è¾ƒå¤š ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet', 'inception']
#æ˜¯å¦ç”¨äººå®¶è®­ç»ƒå¥½çš„ç‰¹å¾æ¥åš
feature_extract = True 
# æ˜¯å¦ç”¨GPUè®­ç»ƒ
train_on_gpu = torch.cuda.is_available()

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')
    
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

â€‹	è¿™é‡Œæ£€æŸ¥ä¸€ä¸‹è‡ªå·±çš„ç”µè„‘æ”¯ä¸æ”¯æŒGPUè®­ç»ƒã€‚ï¼ˆæ‰“å°ä¸€ä¸‹`torch.cuda.is_available()`ï¼‰

```python
def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False
model_ft = models.resnet152() # modelsæ˜¯torchvisioné‡Œçš„åˆé›†ï¼Œæ ‡è¯†å·²ç»è°ƒå¥½ç³»æ•°çš„æ¨¡å‹

```

â€‹	>model_ft

```python
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
...
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
```

â€‹	æˆ‘ä»¬å«–ä¸€ä¸‹Pytorchç»™çš„Demo:

```python
def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):
    # é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œä¸åŒæ¨¡å‹çš„åˆå§‹åŒ–æ–¹æ³•ç¨å¾®æœ‰ç‚¹åŒºåˆ«
    model_ft = None
    input_size = 0

    if model_name == "resnet":
        """ Resnet152
        """
        model_ft = models.resnet152(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.fc.in_features
        model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, 102),
                                   nn.LogSoftmax(dim=1))
        input_size = 224

    elif model_name == "alexnet":
        """ Alexnet
        """
        model_ft = models.alexnet(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier[6].in_features
        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)
        input_size = 224

    elif model_name == "vgg":
        """ VGG11_bn
        """
        model_ft = models.vgg16(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier[6].in_features
        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)
        input_size = 224

    elif model_name == "squeezenet":
        """ Squeezenet
        """
        model_ft = models.squeezenet1_0(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))
        model_ft.num_classes = num_classes
        input_size = 224

    elif model_name == "densenet":
        """ Densenet
        """
        model_ft = models.densenet121(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier.in_features
        model_ft.classifier = nn.Linear(num_ftrs, num_classes)
        input_size = 224

    elif model_name == "inception":
        """ Inception v3
        Be careful, expects (299,299) sized images and has auxiliary output
        """
        model_ft = models.inception_v3(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        # Handle the auxilary net
        num_ftrs = model_ft.AuxLogits.fc.in_features
        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)
        # Handle the primary net
        num_ftrs = model_ft.fc.in_features
        model_ft.fc = nn.Linear(num_ftrs,num_classes)
        input_size = 299

    else:
        print("Invalid model name, exiting...")
        exit()

    return model_ft, input_size
```

â€‹	è®¾ç½®ä¸€ä¸‹å“ªäº›å±‚éœ€è¦è®­ç»ƒï¼š

```python
model_ft, input_size = initialize_model(model_name, 102, feature_extract, use_pretrained=True)

#GPUè®¡ç®—
model_ft = model_ft.to(device)

#Â æ¨¡å‹ä¿å­˜
filename='checkpoint.pth'

# æ˜¯å¦è®­ç»ƒæ‰€æœ‰å±‚
params_to_update = model_ft.parameters()
print("Params to learn:")
if feature_extract:
    params_to_update = []
    for name,param in model_ft.named_parameters():
        if param.requires_grad == True:
            params_to_update.append(param)
            print("\t",name)
else:
    for name,param in model_ft.named_parameters():
        if param.requires_grad == True:
            print("\t",name)
```

â€‹	è®¾ç½®ä¸€ä¸‹ä¼˜åŒ–å™¨ï¼š

```python
# ä¼˜åŒ–å™¨è®¾ç½®
optimizer_ft = optim.Adam(params_to_update, lr=1e-2)
scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)#å­¦ä¹ ç‡æ¯7ä¸ªepochè¡°å‡æˆåŸæ¥çš„1/10
#æœ€åä¸€å±‚å·²ç»LogSoftmax()äº†ï¼Œæ‰€ä»¥ä¸èƒ½nn.CrossEntropyLoss()æ¥è®¡ç®—äº†ï¼Œnn.CrossEntropyLoss()ç›¸å½“äºlogSoftmax()å’Œnn.NLLLoss()æ•´åˆ
criterion = nn.NLLLoss()
```

â€‹	è¿™ä¸ªå‡½æ•°æ˜¯è®­ç»ƒæ¨¡å—ï¼Œç›´æ¥æ‹¿å»ç”¨

```
def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False,filename=filename):
    since = time.time()
    best_acc = 0
    """
    checkpoint = torch.load(filename)
    best_acc = checkpoint['best_acc']
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    model.class_to_idx = checkpoint['mapping']
    """
    model.to(device)

    val_acc_history = []
    train_acc_history = []
    train_losses = []
    valid_losses = []
    LRs = [optimizer.param_groups[0]['lr']]

    best_model_wts = copy.deepcopy(model.state_dict())

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        # è®­ç»ƒå’ŒéªŒè¯
        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()  # è®­ç»ƒ
            else:
                model.eval()   # éªŒè¯

            running_loss = 0.0
            running_corrects = 0

            # æŠŠæ•°æ®éƒ½å–ä¸ªé
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # æ¸…é›¶
                optimizer.zero_grad()
                # åªæœ‰è®­ç»ƒçš„æ—¶å€™è®¡ç®—å’Œæ›´æ–°æ¢¯åº¦
                with torch.set_grad_enabled(phase == 'train'):
                    if is_inception and phase == 'train':
                        outputs, aux_outputs = model(inputs)
                        loss1 = criterion(outputs, labels)
                        loss2 = criterion(aux_outputs, labels)
                        loss = loss1 + 0.4*loss2
                    else:#resnetæ‰§è¡Œçš„æ˜¯è¿™é‡Œ
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, 1)

                    # è®­ç»ƒé˜¶æ®µæ›´æ–°æƒé‡
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # è®¡ç®—æŸå¤±
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)
            
            
            time_elapsed = time.time() - since
            print('Time elapsed {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))
            

            # å¾—åˆ°æœ€å¥½é‚£æ¬¡çš„æ¨¡å‹
            if phase == 'valid' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                state = {
                  'state_dict': model.state_dict(),
                  'best_acc': best_acc,
                  'optimizer' : optimizer.state_dict(),
                }
                torch.save(state, filename)
            if phase == 'valid':
                val_acc_history.append(epoch_acc)
                valid_losses.append(epoch_loss)
                scheduler.step(epoch_loss)
            if phase == 'train':
                train_acc_history.append(epoch_acc)
                train_losses.append(epoch_loss)
        
        print('Optimizer learning rate : {:.7f}'.format(optimizer.param_groups[0]['lr']))
        LRs.append(optimizer.param_groups[0]['lr'])
        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # è®­ç»ƒå®Œåç”¨æœ€å¥½çš„ä¸€æ¬¡å½“åšæ¨¡å‹æœ€ç»ˆçš„ç»“æœ
    model.load_state_dict(best_model_wts)
    return model, val_acc_history, train_acc_history, valid_losses, train_losses, LRs 
```

â€‹	ç°åœ¨è°ƒç”¨ä¸€ä¸‹ï¼š

```
model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs  = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs=20, is_inception=(model_name=="inception"))
```

```
Epoch 0/19
----------
Time elapsed 1m 47s
train Loss: 10.4009 Acc: 0.3141
Time elapsed 1m 56s
valid Loss: 8.2376 Acc: 0.4939
d:\Miniconda\envs\PyEnvTorch\lib\site-packages\torch\optim\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Optimizer learning rate : 0.0010000

Epoch 1/19
----------
Time elapsed 3m 42s
train Loss: 2.1648 Acc: 0.7053
Time elapsed 3m 51s
valid Loss: 3.8922 Acc: 0.5733
Optimizer learning rate : 0.0100000

Epoch 2/19
----------
Time elapsed 5m 41s
train Loss: 9.7510 Acc: 0.4733
Time elapsed 5m 50s
valid Loss: 11.1601 Acc: 0.5110
Optimizer learning rate : 0.0010000

Epoch 3/19
----------
Time elapsed 7m 39s
train Loss: 2.7650 Acc: 0.7486
Time elapsed 7m 49s
valid Loss: 4.8973 Acc: 0.6443
Optimizer learning rate : 0.0100000
...
Optimizer learning rate : 0.0010000

Training complete in 39m 6s
Best val Acc: 0.718826
```

â€‹	å†è®­ç»ƒä¸€ä¸‹ï¼š

```
for param in model_ft.parameters():
    param.requires_grad = True

# å†ç»§ç»­è®­ç»ƒæ‰€æœ‰çš„å‚æ•°ï¼Œå­¦ä¹ ç‡è°ƒå°ä¸€ç‚¹
optimizer = optim.Adam(params_to_update, lr=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

# æŸå¤±å‡½æ•°
criterion = nn.NLLLoss()
```

â€‹	å°†ä¸Šä¸€è½®çš„ç»“æœå–å‡ºæ¥ï¼š

```
# Load the checkpoint

checkpoint = torch.load(filename)
best_acc = checkpoint['best_acc']
model_ft.load_state_dict(checkpoint['state_dict'])
optimizer.load_state_dict(checkpoint['optimizer'])
#model_ft.class_to_idx = checkpoint['mapping']
```

```
model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs  = train_model(model_ft, dataloaders, criterion, optimizer, num_epochs=10, is_inception=(model_name=="inception"))
```

```
Epoch 0/9
----------
Time elapsed 3m 16s
train Loss: 3.0328 Acc: 0.8211
Time elapsed 3m 25s
valid Loss: 7.1805 Acc: 0.7298
d:\Miniconda\envs\PyEnvTorch\lib\site-packages\torch\optim\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Optimizer learning rate : 0.0010000

Epoch 1/9
----------
Time elapsed 6m 45s
train Loss: 2.7197 Acc: 0.8295
Time elapsed 6m 56s
valid Loss: 5.8508 Acc: 0.7665
Optimizer learning rate : 0.0010000

Epoch 2/9
----------
Time elapsed 10m 16s
train Loss: 2.8115 Acc: 0.8239
Time elapsed 10m 26s
valid Loss: 8.6120 Acc: 0.6724
Optimizer learning rate : 0.0010000

Epoch 3/9
----------
Time elapsed 13m 44s
train Loss: 2.6436 Acc: 0.8321
Time elapsed 13m 55s
valid Loss: 8.3770 Acc: 0.6797
Optimizer learning rate : 0.0010000
...
Optimizer learning rate : 0.0010000

Training complete in 34m 39s
Best val Acc: 0.766504
```

### æµ‹è¯•

```
model_ft, input_size = initialize_model(model_name, 102, feature_extract, use_pretrained=True)

# GPUæ¨¡å¼
model_ft = model_ft.to(device)

#Â ä¿å­˜æ–‡ä»¶çš„åå­—
filename='./checkpoint.pth'

# åŠ è½½æ¨¡å‹
checkpoint = torch.load(filename)
best_acc = checkpoint['best_acc']
model_ft.load_state_dict(checkpoint['state_dict'])
```

### æµ‹è¯•æ•°æ®é¢„å¤„ç†

- æµ‹è¯•æ•°æ®å¤„ç†æ–¹æ³•éœ€è¦è·Ÿè®­ç»ƒæ—¶ä¸€ç›´æ‰å¯ä»¥ 
- cropæ“ä½œçš„ç›®çš„æ˜¯ä¿è¯è¾“å…¥çš„å¤§å°æ˜¯ä¸€è‡´çš„
- æ ‡å‡†åŒ–æ“ä½œä¹Ÿæ˜¯å¿…é¡»çš„ï¼Œç”¨è·Ÿè®­ç»ƒæ•°æ®ç›¸åŒçš„meanå’Œstd,ä½†æ˜¯éœ€è¦æ³¨æ„ä¸€ç‚¹è®­ç»ƒæ•°æ®æ˜¯åœ¨0-1ä¸Šè¿›è¡Œæ ‡å‡†åŒ–ï¼Œæ‰€ä»¥æµ‹è¯•æ•°æ®ä¹Ÿéœ€è¦å…ˆå½’ä¸€åŒ–
- æœ€åä¸€ç‚¹ï¼ŒPyTorchä¸­é¢œè‰²é€šé“æ˜¯ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œè·Ÿå¾ˆå¤šå·¥å…·åŒ…éƒ½ä¸ä¸€æ ·ï¼Œéœ€è¦è½¬æ¢

```
def process_image(image_path):
    # è¯»å–æµ‹è¯•æ•°æ®
    img = Image.open(image_path)
    # Resize,thumbnailæ–¹æ³•åªèƒ½è¿›è¡Œç¼©å°ï¼Œæ‰€ä»¥è¿›è¡Œäº†åˆ¤æ–­
    if img.size[0] > img.size[1]:
        img.thumbnail((10000, 256))
    else:
        img.thumbnail((256, 10000))
    # Cropæ“ä½œ
    left_margin = (img.width-224)/2
    bottom_margin = (img.height-224)/2
    right_margin = left_margin + 224
    top_margin = bottom_margin + 224
    img = img.crop((left_margin, bottom_margin, right_margin,   
                      top_margin))
    # ç›¸åŒçš„é¢„å¤„ç†æ–¹æ³•
    img = np.array(img)/255
    mean = np.array([0.485, 0.456, 0.406]) #provided mean
    std = np.array([0.229, 0.224, 0.225]) #provided std
    img = (img - mean)/std
    
    # æ³¨æ„é¢œè‰²é€šé“åº”è¯¥æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®
    img = img.transpose((2, 0, 1))
    
    return img
```

```python
def imshow(image, ax=None, title=None):
    """å±•ç¤ºæ•°æ®"""
    if ax is None:
        fig, ax = plt.subplots()
    
    # é¢œè‰²é€šé“è¿˜åŸ
    image = np.array(image).transpose((1, 2, 0))
    
    # é¢„å¤„ç†è¿˜åŸ
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    image = std * image + mean
    image = np.clip(image, 0, 1)
    
    ax.imshow(image)
    ax.set_title(title)
    
    return ax
```

```python
image_path = 'image_06621.jpg' # æŠ“ä¸€å¼ çœ‹çœ‹æ•ˆæœ
img = process_image(image_path)
imshow(img)
```

![image-20240425194930656](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240425194930656.png)

```python
# å¾—åˆ°ä¸€ä¸ªbatchçš„æµ‹è¯•æ•°æ®
dataiter = iter(dataloaders['valid'])
images, labels = dataiter.__next__()

model_ft.eval()

if train_on_gpu:
    output = model_ft(images.cuda())
else:
    output = model_ft(images)
```

â€‹	outputè¡¨ç¤ºå¯¹ä¸€ä¸ªbatchä¸­æ¯ä¸€ä¸ªæ•°æ®å¾—åˆ°å…¶å±äºå„ä¸ªç±»åˆ«çš„å¯èƒ½æ€§, ä¸‹ä¸€æ­¥çœ‹çœ‹ç»“æœå¦‚ä½•

```python
_, preds_tensor = torch.max(output, 1)

preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())
```

### çœ‹çœ‹é¢„æµ‹çš„ç»“æœ

```python
fig=plt.figure(figsize=(20, 20))
columns =4
rows = 2

for idx in range (columns*rows):
    ax = fig.add_subplot(rows, columns, idx+1, xticks=[], yticks=[])
    plt.imshow(im_convert(images[idx]))
    ax.set_title("{} ({})".format(cat_to_name[str(preds[idx])], cat_to_name[str(labels[idx].item())]),
                 color=("green" if cat_to_name[str(preds[idx])]==cat_to_name[str(labels[idx].item())] else "red"))
plt.show()
```

![image-20240425195105299](./Pytorchå­¦ä¹ ä¹‹è·¯/image-20240425195105299.png)